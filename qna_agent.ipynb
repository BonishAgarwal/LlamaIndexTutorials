{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"78d406f975874c20ac2298c6d83e2f7d\"\n",
    "azure_endpoint = \"https://nitoropenai.openai.azure.com/\"\n",
    "api_version = \"2022-12-01\"\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    model=\"text-davinci-003\",\n",
    "    deployment_name=\"text-davinci-003\",\n",
    "    api_key=api_key,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "\n",
    "# using azure-openai-embeddings\n",
    "# embed_model = AzureOpenAIEmbedding(\n",
    "#     model=\"text-embedding-ada-002\",\n",
    "#     deployment_name=\"text-embedding-ada-002\",\n",
    "#     api_key=api_key,\n",
    "#     azure_endpoint=azure_endpoint,\n",
    "#     api_version=api_version,\n",
    "# )\n",
    "\n",
    "# using hugging-face-embeddings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-readers-file in ./llama-venv/lib/python3.10/site-packages (0.1.5)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in ./llama-venv/lib/python3.10/site-packages (from llama-index-readers-file) (0.10.12)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in ./llama-venv/lib/python3.10/site-packages (from llama-index-readers-file) (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./llama-venv/lib/python3.10/site-packages (from llama-index-readers-file) (4.12.3)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in ./llama-venv/lib/python3.10/site-packages (from llama-index-readers-file) (4.0.2)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in ./llama-venv/lib/python3.10/site-packages (from llama-index-readers-file) (1.23.25)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./llama-venv/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\n",
      "Requirement already satisfied: dataclasses-json in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (0.6.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.12.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2.0.27)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (3.8.1)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.0.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (0.9.0)\n",
      "Requirement already satisfied: httpx in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (0.27.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (3.2.1)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (0.1.13)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (10.2.0)\n",
      "Requirement already satisfied: pandas in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2.2.0)\n",
      "Requirement already satisfied: numpy in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.26.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (4.66.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.6.0)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.2.14)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2.31.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in ./llama-venv/lib/python3.10/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (3.9.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in ./llama-venv/lib/python3.10/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file) (1.23.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./llama-venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./llama-venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (6.0.5)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./llama-venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./llama-venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./llama-venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./llama-venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.9.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in ./llama-venv/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.16.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in ./llama-venv/lib/python3.10/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2.6.1)\n",
      "Requirement already satisfied: sniffio in ./llama-venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.3.0)\n",
      "Requirement already satisfied: certifi in ./llama-venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2024.2.2)\n",
      "Requirement already satisfied: idna in ./llama-venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (3.6)\n",
      "Requirement already satisfied: anyio in ./llama-venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./llama-venv/lib/python3.10/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./llama-venv/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (0.14.0)\n",
      "Requirement already satisfied: joblib in ./llama-venv/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.3.2)\n",
      "Requirement already satisfied: click in ./llama-venv/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./llama-venv/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./llama-venv/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./llama-venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./llama-venv/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2.2.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./llama-venv/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./llama-venv/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./llama-venv/lib/python3.10/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (3.20.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./llama-venv/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./llama-venv/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./llama-venv/lib/python3.10/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2024.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./llama-venv/lib/python3.10/site-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.2.0)\n",
      "Requirement already satisfied: packaging>=17.0 in ./llama-venv/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (23.2)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in ./llama-venv/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (2.16.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./llama-venv/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./llama-venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install llama-index-readers-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "reader = PyMuPDFReader()\n",
    "documents = reader.load_data(\"./docs/finance.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using chromadb vector storage\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db_1\")\n",
    "chroma_collection = db.get_or_create_collection(\"chroma_coll_finance\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using default vector storage\n",
    "\n",
    "# index = VectorStoreIndex.from_documents(documents)\n",
    "# index.storage_context.persist(persist_dir=\"./qna_storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='80b1bce8-ba7c-423f-8feb-2651bef2b723', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '1'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Revolutionizing Finance with LLMs: An\\nOverview of Applications and Insights\\nHuaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang,\\nPeng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai,\\nNinghao Liu, Tianming Liu†∗†\\nJanuary 23, 2024\\nAbstract\\nIn recent years, Large Language Models (LLMs) like ChatGPT have\\nseen considerable advancements and have been applied in diverse fields.\\nBuilt on the Transformer architecture, these models are trained on exten-\\nsive datasets, enabling them to understand and generate human language\\neffectively. In the financial domain, the deployment of LLMs is gaining\\nmomentum. These models are being utilized for automating financial re-\\nport generation, forecasting market trends, analyzing investor sentiment,\\nand offering personalized financial advice. Leveraging their natural lan-\\nguage processing capabilities, LLMs can distill key insights from vast fi-\\nnancial data, aiding institutions in making informed investment choices\\nand enhancing both operational efficiency and customer satisfaction. In\\nthis study, we provide a comprehensive overview of the emerging inte-\\ngration of LLMs into various financial tasks. Additionally, we conducted\\nholistic tests on multiple financial tasks through the combination of natu-\\nral language instructions. Our findings show that GPT-4 effectively follow\\nprompt instructions across various financial tasks. This survey and evalu-\\nation of LLMs in the financial domain aim to deepen the understanding of\\nLLMs’ current role in finance for both financial practitioners and LLM re-\\nsearchers, identify new research and application prospects, and highlight\\nhow these technologies can be leveraged to solve practical challenges in\\nthe finance industry.\\n∗†Corresponding authors: Tianming Liu\\n†Huanqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen\\nXu, Haixing Dai, Lin Zhao, Ninghao Liu, Tianming are with the School of Computing, The\\nUniversity of Georgia, Athens 30602, USA. Gengchen Mai is with the Department of Geogra-\\nphy, University of Georgia, Athens 30602, USA.\\n1\\narXiv:2401.11641v1  [cs.CL]  22 Jan 2024\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='12a09a5a-62bf-46f0-bef8-1f8aff941cab', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Figure 1: An overview of the LLMs’ capacities in financial tasks.\\n1\\nIntroduction\\nOver the past few years, LLMs such as OpenAI’s GPT family have made sig-\\nnificant advances in the field of natural language processing (NLP). The de-\\nvelopment of these models marks an important milestone in AI technology for\\nunderstanding and generating natural language. With increased computational\\npower and improved algorithms, LLMs has demonstrated amazing capabilities\\nin understanding complex contexts, answering questions, and writing content.\\nEspecially in the finance domain, these capabilities of LLMs are gradually show-\\ning their great potential [18, 125, 126].\\nFinance is a highly specialized and complex field that involves a great deal\\nof data analysis, prediction, and decision making.\\nLLM’s ability to process\\nlarge-scale text data makes it a promising application in the financial field. For\\nexample, by analyzing financial reports, market news, investor communications,\\netc., LLMs can provide insights into market trends, perform risk assessments,\\nand even assist in investment decisions. In addition, LLMs can process natural\\nlanguage queries and provide instant financial advice and support, which is a\\nbig step forward for the financial services industry [28, 66, 125].\\nHowever, applying LLMs to the financial sector also faces several challenges.\\nFirst, data in the financial domain is highly specialized and complex. Financial\\nterminology, regulations, and market dynamics require a high level of model\\ncomprehension. In addition, financial decision-making usually involves high risk,\\nwhich requires a high degree of accuracy and reliability in prediction. Therefore,\\nit is a major challenge to ensure that the output of LLMs is both accurate and\\n2\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c1bdd320-34c9-4590-9f9a-bc943fc1cc74', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '3'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='reliable [42].\\nTo address these issues, researchers and developers are continuously refining\\nthe algorithms of LLMs to improve its understanding and processing of special-\\nized domain knowledge. With a large amount of specialized training data, the\\nmodel can better grasp specific knowledge in the financial domain. At the same\\ntime, the combination of expert systems and manual review mechanisms can\\nfurther improve the accuracy and reliability of the model’s application in the\\nfinancial domain [131].\\nOverall, large-scale language models are gradually becoming a powerful tool\\nfor dealing with financial problems.\\nThey are not only able to process and\\nanalyze large amounts of data, but also provide in-depth insights and recom-\\nmendations [125]. Although there are some challenges in the application process,\\nthey are gradually being overcome as technology continues to advance. Looking\\nahead, the application of LLMs in the financial sector will undoubtedly open up\\nmore innovations and opportunities.\\nIn this review, we tackle the key question of how to address the difficul-\\nties inherent in the financial sector while utilizing the successes of LLMs from\\nvarious fields to enhance the finance industry. The significant contributions of\\nthis article are distilled into four primary points, each focusing on the synergy\\nbetween LLMs and financial applications.\\n• We meticulously survey and synthesize existing LLMs for finance liter-\\nature, exploring the latest advancements in four independent task cate-\\ngories: financial engineering, financial forecasting, financial risk manage-\\nment, and financial real-time question answering.\\n• We summarize the primary technical approaches that LLMs offer to the\\nrealm of finance, examine the potential in the investment field, and provide\\na foundational survey for researchers in this domain.\\n• We assess the effectiveness of GPT-4 in various tasks.\\n• We concisely overview of the most significant results from our research,\\ndiscuss the major unresolved issues that should be addressed in subsequent\\nefforts, and offer insights into future directions and possibilities in this\\nfield..\\n2\\nRelated Work\\n2.1\\nLarge Language Models\\nLLMs are primarily built upon the Transformer architecture [119], which has\\nbeen central to their ability to handle complex language tasks. The Transformer\\nmodel is structured with two key components: the Encoder and the Decoder,\\neach consisting of multiple layers of self-attention and feed-forward neural net-\\nworks. This architecture facilitates effective management of long-range depen-\\ndencies within sequences.\\n3\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='09c44a6f-d1c8-44bf-821c-93a687fc66b7', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '4'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Figure 2: The Ability of LLMs in Financial Tasks\\nAttention(q, k, v) = softmax( qk\\n√dk\\n)v\\n(1)\\nSelf-attention is characterized by its use of queries (Q), keys (K), and values\\n(V), three vectors derived from the input data.\\nEach element in the input\\nsequence is transformed into these three vectors through linear transformation.\\nThe self-attention mechanism then computes the attention scores by taking the\\ndot product of the query with all keys. These scores determine how much focus\\nor ’attention’ each element in the sequence should have in relation to every\\nother element. The attention scores are normalized using a softmax function,\\nensuring they sum up to one, thus forming a probability distribution. The final\\noutput of the self-attention layer is a weighted sum of the value vectors, where\\nthe weights are the softmax-normalized attention scores. This process allows\\neach output element of the self-attention layer to be a combination of the inputs,\\nwith the weights specifying the amount of attention given to each input element.\\nThe self-attention mechanism’s ability to weigh inputs differently allows LLMs\\nto capture complex relationships in the data, such as long-range dependencies,\\nmaking it exceptionally powerful for tasks that require an understanding of\\ncontext and sequence.\\nThe architecture of LLMs typically falls into one of two categories: Decoder-\\nonly and Encoder-Decoder. Decoder-only models, such as those in the GPT\\nseries [99, 100], generate text in a unidirectional manner [12]. Each token in\\nthe input sequence attends only to preceding tokens, making them well-suited\\nfor tasks like text generation. The Encoder-Decoder models, like T5 [102] and\\nBART [56], feature separate mechanisms for encoding input sequences and de-\\ncoding them into target sequences. This design allows them to handle a broader\\nrange of tasks, including both generation and comprehension.\\nToken generation in LLMs is a vital process, involving vocabulary creation,\\nprobability prediction, and techniques like beam search for sequence generation.\\n4\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='8b5d2dd2-fbd3-4121-8c49-21fd7e8a5b63', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '5'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Vocabulary in LLMs is typically constructed using methods like Byte-Pair En-\\ncoding (BPE) [38], which allows the model to break down words into subword\\nunits.\\nThis method aids in managing the model’s vocabulary size, ensuring\\nefficient handling of rare words and morphemes.\\nDuring the token generation process, LLMs predict the probability of each\\ntoken given the context provided by the input sequence.\\nThis is typically\\nachieved through a softmax layer that converts the output logits into a proba-\\nbility distribution over the vocabulary. The model selects tokens based on these\\nprobabilities, either choosing the most likely next token (greedy decoding) or\\nleveraging techniques like beam search. Beam search is a decoding strategy [36]\\nthat maintains a fixed number of candidate sequences at each step. It expands\\neach candidate by one token at a time, computes the probability of each expan-\\nsion, and keeps only the most likely sequences. This method balances between\\nfinding the most probable sequence and maintaining a diverse set of candidate\\nsequences, leading to more coherent and contextually appropriate outputs.\\nThe capabilities inherent in the Transformer architecture and token genera-\\ntion processes of LLMs have facilitated their application across a wide range of\\ndomains. For instance, in Natural Language Generation (NLG), the Decoder-\\nonly models excel at producing contextually relevant text, suitable for creative\\nwriting and automated report generation.\\nEncoder-Decoder models, due to\\ntheir bidirectional processing ability, are highly effective in tasks like machine\\ntranslation, capable of converting input from one language to another while\\npreserving semantic integrity [140].\\nFor example, in conversational AI, LLMs power sophisticated chatbots and\\nvirtual assistants [55, 68], capable of generating human-like responses in real-\\ntime. Their ability to understand and generate language fluently makes them\\nideal for customer service automation, interactive learning platforms, and per-\\nsonalized communication tools.\\nLLMs also play a crucial role in information extraction and summariza-\\ntion [69, 111], distilling lengthy documents into concise, informative summaries.\\nThis application is particularly beneficial in fields like journalism and academic\\nresearch, where quick assimilation of information is essential.\\nFurthermore, the sophisticated understanding of context and language nu-\\nances allows LLMs to perform sentiment analysis [68, 69], including financial\\nsentiment analysis [136]. This capability is widely used in brand monitoring,\\nmarket research, and social media analysis, providing insights into public opin-\\nion and consumer behavior.\\nOverall, the technical intricacies of LLMs, from their architectural design\\nto their token generation methods, underpin a broad spectrum of applications\\n[26, 40, 44, 65, 77]. These models not only enhance existing processes but also\\nopen up new possibilities in the way we interact with and process language.\\n2.2\\nNamed Entity Recognition\\nNamed Entity Recognition (NER) is a key technology in the field of NLP, used\\nto identify and classify entities with specific meanings from text, such as names,\\n5\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='24a5bcfa-ab20-4a2e-b86e-d5de84cad96a', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '6'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='places, organizations, time expressions, financial terms, etc.\\nNER plays an\\nimportant role in information extraction, question-answering systems, content\\nanalysis, knowledge graph construction, and other fields [93]. There are three\\nmain mainstream approaches to solving the NER, namely Rule-Based methods,\\nMachine Learning-Based methods, and Deep Learning-Based methods.\\n[31]\\nRule-based systems operate based on identifying entities using predefined rules\\nand patterns, such as using a dictionary of place names to recognize locations.\\nIt is easily interpretable and does not require training data. While reliant on\\nexpert knowledge, these methods have limited flexibility and scalability [57].\\nMachine Learning-Based Methods: These methods, such as Support Vector Ma-\\nchines (SVM) and Random Forests, learn to recognize entities through training\\ndatasets based on manually selected features. They offer more flexibility than\\nrule-based methods but require extensive annotated data [32]. Deep learning\\ntechniques for tagging sequences make use of word and character representa-\\ntions that are distributed, by training on sentence or sequence features in an\\nend-to-end manner. These methods mainly use BiLSTM structures or networks\\nbased on self-attention. They frequently use a Conditional Random Field (CRF)\\nlayer for decoding tags, aiding in the comprehension of label interdependencies.\\nLeveraging these capabilities, deep learning approaches are highly effective in\\nmanaging intricate patterns and extensive data sets [30, 85]. NER is widely\\nused in the financial field, it can be applied for information extraction (extract-\\ning key details about companies, stocks, and market events from financial news\\nand reports), compliance monitoring (automatically identifying and overseeing\\nsensitive entities in financial documents, like money laundering and fraud), and\\ninvestment decision support (providing data support for investment decisions by\\nanalyzing entities and events in market news and reports). These applications\\nunderscore the vital role of NER in enhancing efficiency, ensuring compliance,\\nand supporting strategic decisions [139].\\n2.3\\nSentiment Analysis\\nIn contemporary financial market forecasting, especially regarding Bitcoin trad-\\ning, the significance of sentiment analysis has been corroborated through numer-\\nous academic studies [10, 58, 133]. This research area primarily bifurcates into\\ntwo methodological categories: lexicon-based and machine-learning approaches,\\nboth pivotal in discerning market trends.\\nLexicon-Based Methodology: Within this category, approaches are sub-\\ndivided into dictionary and corpus-based strategies. A notable instance is the\\nmodel developed by Dev Shah et al. [109], which utilizes the ’pattern’ Python\\nlibrary for transforming textual data into numerical vectors. This process in-\\nvolves compiling sentiment scores by quantifying the occurrence of positive and\\nnegative words. However, this model faces limitations due to its unweighted\\nsentiment scoring for individual words, potentially leading to inaccuracies in\\nmirroring the actual market sentiment.\\nMachine Learning Techniques: These are split into unsupervised and\\nsupervised learning. The unsupervised model by M.S. Usha et al. [118], which\\n6\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d8948713-1493-43fc-ae30-67e491c94128', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '7'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='leverages the Gibbs sampling algorithm, excels in identifying sentiment and top-\\nics simultaneously. Yet, its inefficacy in capturing neutral sentiments poses a\\nconstraint. In contrast, the supervised approach by D.K. Kirange et al.\\n[52]\\nfocuses on classifying emotions in news content to determine sentiment polarity,\\nemploying algorithms such as Naive Bayes, SVM, and KNN, with the latter\\nshowing optimal accuracy.\\nMoreover, Sneh Kalra et al.\\n[48] introduced a\\nmodel that synergizes Naive Bayes sentiment analysis with adjacent date stock\\nvariance data from Yahoo Finance, although it is somewhat limited by its re-\\nliance on a single data source. Xiadong Li et al. [60] proposed a novel deep\\nlearning-based stock prediction system that fuses sentiment analysis with tech-\\nnical stock indicators. Additionally, the field has seen diverse methodologies\\nsuch as specialized NLP sub-module designs for sentiment analysis [95, 108],\\nthe application of N-gram and Naive Bayes Algorithms [50], dictionary-based\\nsentiment analysis\\n[49], and mood classification paired with daily sentiment\\nscoring [6, 91]. Time series analysis models have also found their application\\nin this area [92].\\nThese varied methodologies underscore the complexity and multidimension-\\nality of sentiment analysis in financial forecasting, particularly in the context of\\nnews analysis. Each approach offers a unique lens through which market trends\\ncan be decoded and anticipated, demonstrating the intricate interplay between\\nmarket sentiment and financial news analysis.\\n2.4\\nQuestion Answering\\nLarge language models (LLMs), such as GPT-4, have demonstrated remarkable\\ncapabilities in question answering [124], mainly due to their complex architec-\\nture and large amounts of training data.\\nLLMs obtains broad knowledge coverage by analyzing large amounts of text\\ndata on the Internet. They can answer questions ranging from general knowledge\\nto specialized fields such as finance, history, science, technology, art, and more\\n[14, 15, 25, 40, 41, 61, 63, 68, 71, 73, 76, 80, 86, 104, 111, 114]. LLMs can\\nunderstand complex queries [44, 59, 73, 79, 103, 105, 115, 121, 127]. Whether it’s\\nlong sentences, ambiguous questions, or questions that require the synthesis of\\ndifferent information sources, LLMs can handle it and provide relevant answers\\n[43, 68, 115]. LLM can maintain contextual coherence in conversations. This\\nmeans it can understand and answer subsequent questions based on previous\\nconversations, providing more accurate and relevant information [68, 74, 138].\\nTop LLMs often have multilingual capabilities and can understand and answer\\nquestions in different languages [116], which allows them to serve a wider user\\nbase.\\nLLMs have exhibited remarkable capabilities in advanced reasoning.\\nFor\\ninstance, GPT-4 showcases its ability for common-sense reasoning by leveraging\\nin-context learning.\\nMoreover, the study [124] reveals that when LLMs are\\nprovided with well-structured sequential prompts that break down complex,\\nmulti-step problems, their performance in tasks involving arithmetic, deductive\\nreasoning, and common-sense understanding improves significantly.\\n7\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0dc75924-895e-4e65-be82-5728e446051e', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '8'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='2.5\\nTime Series Forecasting\\nFinancial time series forecasting has traditionally hinged on statistical and\\neconometric methods. Models like ARMA-GARCH have been pivotal in dis-\\ncerning patterns and volatility in financial series [101].Over time, these models\\nhave been refined to better interpret the intricacies of financial markets. Other\\nmethods that have gained prominence include Vector Autoregressive Models\\n(VAM) [145], State-Space Models utilizing Kalman Filters [97], Diffusion Mod-\\nels [34], and Vector Error Correction Model (VECM) [47], forming the bedrock\\nof financial analysis.\\nThe emergence of machine learning has introduced a plethora of models\\nfor financial forecasting. Decision trees and support vector machines, known\\nfor their effectiveness in financial series prediction, have become particularly\\nprominent. Of late, there has been a pivot towards deep learning techniques\\nsuch as Recurrent Neural Networks (RNNs), Convolutional Neural Networks\\n(CNNs), and Transformer models, renowned for their proficiency in unraveling\\ncomplex, non-linear data relationships.\\nThe development of LLMs like GPT-3 [12], GPT-4 [1], and LLaMA [117],\\nhas been a game-changer in the realm of financial time series forecasting. These\\nmodels excel in parsing and interpreting intricate dependencies in diverse data\\nsets, offering outputs that are comprehensible to humans. There has been con-\\nsiderable advancement in this domain, including the conversion of time series\\ndata into textual sequences, the creation of varied prompts for intelligible finan-\\ncial forecasting, and the conceptualization of financial time series as multimodal\\ndata, harnessing the combined strengths of LLMs and computer vision. These\\ndevelopments showcase the dynamic and expanding role of LLMs in financial\\ntime series forecasting, highlighting a field ripe with innovation and exploration\\n[17, 46, 134].\\n2.6\\nMathematical Reasoning\\nMathematical reasoning forms the cornerstone of modern finance, serving as the\\nbedrock upon which complex financial theories, models, and practices are con-\\nstructed. In the realm of finance, mathematical reasoning extends beyond mere\\nnumber crunching; it encompasses the application of mathematical principles to\\nanalyze and solve financial problems, thereby empowering professionals to make\\ninformed decisions, assess risks, and forecast market trends.\\nCentral to mathematical reasoning in finance is the integration and applica-\\ntion of various mathematical disciplines, such as calculus, statistics, probability,\\nand linear algebra. These mathematical frameworks enable finance profession-\\nals to devise and interpret financial models, assess investment strategies, and\\noptimize portfolios. Calculus, for instance, is pivotal in modeling the dynamic\\nbehavior of markets and in calculating derivatives, which are key in risk man-\\nagement and the pricing of financial instruments [11]. Moreover, statistics and\\nprobability are indispensable in evaluating risks and returns, aiding in asset\\nvaluation and the development of predictive models [54].\\n8\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='28ff5d5b-876f-4618-adc4-962aaecbbbb5', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '9'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Furthermore, mathematical reasoning in finance is dynamic and continually\\nevolves with the emergence of new theories and the advent of technological ad-\\nvancements. The inception of quantitative finance, which amalgamates mathe-\\nmatical finance, numerical methods, and computer simulations, has transformed\\nthe industry. This interdisciplinary approach has led to the creation of intricate\\nmodels for options pricing, risk management, and algorithmic trading, thereby\\nenhancing the precision and efficiency of financial operations [13].\\nAs we traverse an era marked by increasing complexity and interconnec-\\ntivity in financial markets, the significance of mathematical reasoning becomes\\nincreasingly critical. It not only furnishes finance professionals with the neces-\\nsary tools for understanding and innovation but also instills a rigorous analytical\\nframework, which is vital amidst financial uncertainties. Whether it’s in the val-\\nuation of complex derivatives, the formulation of robust financial models, or the\\nstrategic management of investment portfolios, mathematical reasoning remains\\nan essential component in the repertoire of contemporary finance [24].\\n3\\nScope of Finance Tasks\\n3.1\\nFinancial Engineering\\nFinancial Engineering is a multidisciplinary field that combines finance, math-\\nematics, and computer science to create and implement innovative financial\\nstrategies and products. LLMs assist in Financial Engineering by enhancing\\ntwo key subtasks: Quantitative Trading and Portfolio Optimization.\\n3.1.1\\nQuantitative Trading\\nQuantitative trading has traditionally relied on mathematical and statistical\\nmodels to drive investment decisions, often centering around historical data\\nand predefined algorithmic strategies. This approach, while effective in certain\\nmarket conditions, faces challenges in dynamic and complex market environ-\\nments. Traditional quantitative models can struggle to adapt quickly to new\\ninformation, particularly when it comes to unstructured data sources like news\\narticles, social media, and financial reports. These sources contain valuable sen-\\ntiment and opinion-based information that standard quantitative methods may\\noverlook [66, 135].\\nIn recent years, the emergence of LLMs has opened new avenues in quan-\\ntitative trading. LLMs, with their advanced natural language processing capa-\\nbilities, play a pivotal role in effectively extracting and utilizing such implicit\\nsentiment information in investment strategies. By analyzing vast amounts of\\ntextual data, LLMs can identify subtle, often nuanced sentiments embedded in\\nanalysts’ reports, market news, and financial statements. These sentiments are\\ncrucial as they often represent the collective market sentiment and can precede\\nmajor market movements.\\nAnalysts’ reports, for instance, are a goldmine of insights but are often laden\\nwith implicit sentiments that the analysts might be reluctant or avoid revealing\\n9\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='91b592d1-1b7a-4cf6-bc01-1ae4349d64f2', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '10'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='directly. LLMs can decipher these subtle cues, providing a more comprehen-\\nsive understanding of market dynamics. This capability extends beyond mere\\nsentiment analysis; it encompasses the understanding of context, the detection\\nof sarcasm, and the interpretation of complex financial jargon, which are often\\nlost in traditional quantitative analysis.\\nThe integration of LLMs into quantitative trading strategies represents a\\nsignificant advancement in the field. It allows for a more holistic approach to\\ninvestment decisions, one that combines the precision of quantitative models\\nwith the nuanced understanding of market sentiments. This synergy not only\\nenhances the robustness of trading strategies but also provides a competitive\\nedge in rapidly changing market conditions. As the financial markets continue to\\nevolve, the role of LLMs in quantitative trading is poised to become increasingly\\nvital, marking a paradigm shift in how investment decisions are made. [123]\\n3.1.2\\nPortfolio Optimization\\nTraditional portfolio optimization, grounded in the principles of modern port-\\nfolio theory, seeks to balance risk against return, typically relying on historical\\nmarket data and statistical analysis [98]. This approach, while systematic, of-\\nten encounters challenges in rapidly evolving markets where historical data may\\nnot adequately predict future trends. Additionally, traditional models may not\\nfully account for complex, real-world factors like geopolitical events or sudden\\nmarket shifts, potentially leading to suboptimal asset allocations [3].\\nThe integration of LLMs in portfolio optimization heralds a significant ad-\\nvancement in addressing these challenges. LLMs excel in processing and analyz-\\ning vast amounts of unstructured data, including market reports, news articles,\\nand financial statements, providing deeper insights and supplementary analysis\\ncrucial for risk assessment. These models can uncover subtle market sentiments\\nand emerging trends hidden in textual data, offering a more nuanced view of po-\\ntential risks and opportunities. By augmenting quantitative data with qualita-\\ntive insights derived from LLMs, investors can achieve a more holistic approach\\nto portfolio optimization. This synergy not only enhances the robustness of\\ntraditional models but also equips investors with a more adaptive and informed\\nstrategy in the face of market uncertainties. [45]\\n3.1.3\\nRobo-advisors\\nLeveraging the analytical power of LLMs and artificial intelligence (AI), robo-\\nadvisors are making significant strides in reshaping the world of financial in-\\nvesting. Combining precision, adaptability, and accessibility, these advanced\\nplatforms are quickly becoming popular tools for wealth management and in-\\nvestment advisory services.\\nThe essence of robo-advisors’ appeal lies in their computational power, which\\nallows them to tailor portfolios to the individual user’s circumstances, taking\\ninto account market dynamics and personal risk preferences. The LLMs is crit-\\nical in this context, parsing extensive data sets to discern complex financial\\n10\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9d69aeb4-09ff-4696-ad6c-f1201fdd5b93', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '11'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='market patterns, allowing robo-advisors to provide informed investment guid-\\nance. Throughout the investment cycle, they continuously monitor portfolio\\nperformance, adjusting the balance between expected returns and user-defined\\nrisk thresholds [16].\\nA key benefit of robo-advisors is their ability to flexibly update investment\\nstrategies to reflect changes in the market, a flexibility often not available with\\ntraditional investing avenues. The enhanced flexibility can foster greater trust\\nbetween financial advisors and their clients [9].While historical analysis has pri-\\nmarily focused on the algorithmic sophistication and legitimacy of robo-advisors\\n[83], discussion is burgeoning about the psychological factors that guide indi-\\nviduals to use these AI platforms.\\nIn a revealing study of the German robo-advisory market (covering approx-\\nimately 78 assets), they examined approximately 243,000 portfolio pairs along\\nwith customer demographic data. The findings indicate that despite the high\\nlevel of AI-driven sophistication, the personalization aspects of robo-advisory\\nadvice are currently limited [107]. Key factors that influence modern portfolio\\nchoices—such as the amount and nature (beta) of human capital or shadow\\nassets—remain largely unresolved. Recommendations tend to cater to current\\ninvestor biases or regulators’ views on portfolio allocation, which inadvertently\\nlimits the economic potential of robo-advisors while bolstering consumer con-\\nfidence and ensuring regulatory sanctions.\\nThe renaissance of robo-advisory\\nadvice is highlighted by its tendency to eschew complex, customized strategies\\nin favor of more broadly applicable investment principles, for reasons including\\nexplainability to the average user and the need for privacy and data security [7].\\nAll in all, the integration of LLMs with robo-advisory services marks a quan-\\ntum leap in the field of consulting. These AI-centric platforms will revolutionize\\ninvesting and wealth management by connecting intricate financial acumen with\\nthe understanding of ordinary investors, although there is caution about their\\ncurrent scope for customization [90] [96].\\n3.2\\nFinancial Forecasting\\n3.2.1\\nMerge and Acquisition Forecasting\\nIn Mergers and Acquisitions (M&A) forecasting, NLP offers pivotal tools for\\nmining and interpreting vast arrays of textual data [120, 130]. LLMs can adeptly\\nanalyze financial reports, news articles, and press releases to unearth underlying\\ntrends or strategic shifts that may hint at forthcoming M&A activities [129].\\nFurthermore, sentiment analysis, a crucial facet of NLP, scrutinizes market\\ncommentaries and financial reports. This analysis is instrumental in detecting\\nshifts in market sentiment regarding specific companies or sectors, potentially\\nforeshadowing M&A endeavors.\\nAdditionally, LLMs can delve into historical M&A cases and identify linguis-\\ntic and financial patterns that typically precede such corporate actions. This\\nhistorical insight is invaluable in predicting future M&A activities. Lastly, the\\nrole of social media cannot be understated. LLMs can monitor these platforms\\n11\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2c0529ca-5242-4c24-a856-6be6697b6906', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '12'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='for speculative information and public sentiment, often serving as early indica-\\ntors of possible M&A movements.\\n• Analyzing Financial Reports and News Articles: Hypothetical Sce-\\nnario: LLMs analyze the financial reports and news articles surrounding\\ntech giants like Apple (AAPL) and a smaller, innovative tech company\\nlike Roku (ROKU). The analysis reveals a pattern of increasing mentions\\nof collaborative projects and shared technology initiatives, suggesting a\\nstrategic alignment. This could hint at a potential acquisition of Roku\\nby Apple, a move that could significantly expand Apple’s footprint in the\\nstreaming hardware market.\\n• Sentiment Analysis of Market Commentaries: Hypothetical Sce-\\nnario: NLP tools conduct sentiment analysis on market commentaries\\nregarding the pharmaceutical industry.\\nThey detect a positive shift in\\nsentiment towards Merck (MRK) and a smaller biotech firm, BioNTech\\n(BNTX), known for its breakthroughs in mRNA technology. This senti-\\nment shift, coupled with increased collaborative research efforts between\\nthe two, might suggest an impending merger or partnership, aligning\\nMerck’s robust distribution network with BioNTech’s innovative vaccine\\ntechnology.\\n• Examining Historical M&A Patterns: Hypothetical Scenario: An\\nLLM reviews historical M&A cases in the automotive sector, particularly\\nfocusing on Tesla (TSLA) and its past acquisitions of smaller tech compa-\\nnies specializing in autonomous driving technology. By identifying linguis-\\ntic and financial patterns from these cases, such as Tesla’s strategic invest-\\nments in AI technology, the model predicts Tesla’s interest in acquiring\\na company like Mobileye (MBLY), a leader in advanced driver-assistance\\nsystems.\\n• Monitoring Social Media for Speculative Information: Hypothet-\\nical Scenario: LLMs monitor platforms like Twitter and LinkedIn for dis-\\ncussions involving Disney (DIS) and Netflix (NFLX). An uptick in specula-\\ntive discussions about Disney’s interest in enhancing its streaming content\\nand a potential strategic fit with Netflix’s vast content.\\n3.2.2\\nInsolvency Forecasting\\nFor insolvency forecasting, language models can analyze a myriad of textual\\nsources to gauge a company’s financial health accurately. By evaluating financial\\ndisclosures, news articles, and statements from corporate leaders, these models\\ncan detect early signs of financial distress [19].\\nTo complement traditional numerical modeling, these models integrate tex-\\ntual analysis from various reports and news sources into bankruptcy prediction\\nmodels, enhancing their predictive accuracy [2]. The sentiment and tone in cor-\\nporate communications and financial discussions can be meticulously analyzed\\nand reveal early warnings of a company’s deteriorating financial situation.\\n12\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='61ad9284-e2be-4fa2-b6ab-500aba8d5abe', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '13'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Moreover, a critical examination of regulatory filings through NLP can reveal\\nsubtle linguistic or disclosure patterns [51]. These patterns are frequently ob-\\nserved in the prelude to financial difficulties or impending insolvency, providing\\nessential insights for stakeholders and investors.\\n• Analyzing Financial Health of Retail Companies: LLMs can assess\\nthe financial statements of retail companies. They would identify signs of\\nfinancial distress, such as declining sales and increasing debt levels, that\\nmay indicate a risk of insolvency.\\n• Sentiment Analysis in Industry News: Sentiment analysis on news\\narticles and financial reports about a technology firm. If there’s a preva-\\nlent negative sentiment and discussions about liquidity issues or declining\\nmarket share, this could signal financial troubles ahead.\\n• Monitoring Social Media for Consumer Sentiment:\\nLLMs can\\ntrack social media mentions of an automotive company, say, ”AutoDrive\\nMotors”. By analyzing consumer sentiment and discussions about prod-\\nuct issues or declining brand popularity, potential financial struggles could\\nbe anticipated.\\n• Analyzing Credit Ratings and Analyst Reports: A language model\\ncan examine changes in credit ratings and analyst reports to spot trends\\nsuch as credit rating downgrades or negative outlooks by financial analysts\\ncould be early indicators of impending financial difficulties.\\n• Reviewing Legal and Regulatory Filings: LLMs can analyze legal\\nand regulatory filings for a pharmaceutical company. Increases in litigation\\ncases or regulatory fines might be early signs of financial instability.\\n3.2.3\\nMarket Trend Forecast\\nIncorporating GPT-4’s capabilities into market trend analysis represents a sig-\\nnificant leap forward in the application of artificial intelligence within the do-\\nmain of financial forecasting. The endeavor to leverage GPT-4’s API for pre-\\ndicting stock price trajectories is an intricate process that navigates through a\\nconfluence of challenges.\\nHistorically, the academic sphere has gravitated towards econometric mod-\\nels, such as ARIMA [94], and the finance industry has harnessed machine learn-\\ning algorithms to predict stock movements. These methods, while effective to\\na degree, cannot often evolve rapidly with market conditions or explain their\\npredictions transparently.\\nMarket dynamics are notoriously difficult to predict due to their stochastic\\nnature and the multitude of influential variables ranging from macroeconomic\\nindicators to geopolitical events and investor sentiment. These factors are inter-\\ndependent and can exhibit non-linear relationships, presenting a daunting task\\nfor any predictive model. Traditional quantitative models, while robust, often\\n13\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='39c7082d-0842-4c55-9b66-759a99e649bc', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '14'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Figure 3: GPT-4’s forecasting capability on stock price movement\\nstruggle to account for the subtleties of market sentiment and the rapid shifts\\nin global economic landscapes.\\nNLP is increasingly being utilized in market forecasting to complement tra-\\nditional quantitative analysis methods [81]. By processing and interpreting tex-\\ntual data from various sources such as news articles, financial reports, and social\\nmedia, NLP provides valuable insights into market sentiment and trends [129].\\nThis use of NLP helps in uncovering underlying patterns and correlations in\\nmarket behavior that might not be immediately apparent from numerical data\\nalone. In market forecasting, the ability of NLP to quickly analyze large vol-\\numes of text and extract relevant information plays a crucial role in making\\ntimely and informed predictions about market movements.\\nDiverse Data Sources in Market Forecasting with LLMs:\\n• Processing Financial News and Reports: LLMs can quickly digest\\nand analyze extensive financial news and reports, providing a comprehen-\\nsive view of market conditions and potential trends.\\n• Sentiment Analysis of Social Media: By examining social media plat-\\nforms and analyzing the sentiment of posts and tweets, LLMs can gauge\\npublic opinion and investor sentiment, which are crucial indicators of mar-\\nket movements.\\n• Interpreting Economic Indicators: LLMs can interpret textual data\\nrelated to economic indicators such as inflation rates, employment data,\\nand GDP growth, which traditionally influence market forecasts.\\n• Scenario Simulation: Leveraging historical data, LLMs can simulate\\nvarious market conditions and outcomes, aiding in risk assessment and\\ndecision-making processes.\\n• Real-time Data Processing: The ability to process data in real time\\n14\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='53a8f66d-c0cc-498e-8312-d85b90d3141a', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '15'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='allows LLMs to stay abreast of rapid market changes, offering timely in-\\nsights for forecasting.\\nAdvantages and Potential of LLMs in Market Analysis:\\n• Enhanced Predictive Capabilities: By analyzing a broader range of\\ndata sources, LLMs can offer more accurate predictions than traditional\\nnumerical-only methods.\\n• Holistic Market Understanding: The integration of textual data anal-\\nysis provides a more holistic understanding of market dynamics, beyond\\nwhat numerical data alone can offer.\\n• Adaptability to Market Changes: The AI-driven nature of LLMs\\nallows for quick adaptation to new information and changing market sce-\\nnarios.\\n• Customizable Analysis: LLMs can be tailored to focus on specific sec-\\ntors, regions, or types of data, making them versatile tools for various\\nmarket analysis needs.\\n• Reducing Human Bias: By relying on data-driven insights [88], LLMs\\ncan help reduce human bias in market forecasting, leading to more objec-\\ntive and reliable predictions.\\nAs the most powerful language model to date [113, 122], GPT-4 brings to\\nthe table its formidable prowess in processing vast datasets, extracting nuanced\\npatterns, and synthesizing this information to generate predictions. Its capac-\\nity to parse through disparate data sources, including real-time financial news,\\nhistorical price data, and burgeoning trends on social media platforms, allows\\nit to construct a multi-faceted view of market conditions.\\nFurthermore, GPT-4 transcends mere predictive output; it provides the un-\\nderlying rationale for its forecasts, thereby granting investors and analysts a win-\\ndow into the ’thought process’ of the AI. This interpretability is paramount, as\\nit aligns with the rigorous standards of academic research and financial scrutiny,\\nenabling stakeholders to make informed decisions.\\nThe experiment conducted with GPT-4’s API, which culminated in accurate\\nand interpretable outcomes, indicates a paradigm shift. This advancement may\\nredefine predictive analytics in finance, offering a more dynamic, holistic, and\\ntransparent approach to understanding and anticipating market trends. This\\nstudy serves as a testament to the potential of integrating advanced AI into\\nfinancial analysis and the broader implications for future research and practical\\napplications within the industry.\\n3.3\\nFinancial Risk Management\\n3.3.1\\nCredit Scoring\\nThe significance of credit and risk assessments in the financial sector cannot\\nbe overstated, as these evaluations play a crucial role in maintaining financial\\n15\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='358eb24a-6232-4b1a-90c2-13dceefa02e8', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '16'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='stability. Credit assessment not only covers the possibility of assessing an in-\\ndividual borrower’s ability to repay, but also includes a variety of applications\\nsuch as analyzing the risks of potential investments, evaluating the financial\\nhealth of a company, and assisting financial institutions in making decisions\\nabout loan policies and interest rates. Traditionally, financial credit and risk\\nassessment methods have predominantly been rule-based or reliant on machine\\nlearning algorithms [27]. However, these approaches exhibit limited flexibility\\nacross different tasks, often being tailor-made for specific objectives. Conse-\\nquently, they struggle to generalize or integrate knowledge from diverse finan-\\ncial tasks. Moreover, such methods cannot leverage insights transferable across\\nvarious financial activities. The advent of LLMs offers a promising avenue to\\ntranscend these limitations [8, 132, 141]. LLMs, with their prowess in multi-\\ntask learning and few-shot generalization, present an opportunity to redefine\\nthe landscape of financial assessments. Current research is exploring the po-\\ntential of LLMs to identify correlations between disparate financial tasks and\\ngeneralize across them.\\nThis capability marks a potential paradigm shift in\\ncredit and risk evaluation methodologies. The application of LLMs in this do-\\nmain, however, is not without its challenges. For instance, the need to analyze\\ntabular data, which contains symbolic information markedly different from the\\nnatural language data that LLMs are typically trained on, presents a signifi-\\ncant hurdle. Additionally, ensuring that these models avoid biases in sensitive\\nattributes such as age or gender is paramount. Despite these challenges, LLMs\\noffer considerable advantages in processing and analyzing large volumes of tex-\\ntual data, such as loan applications and transaction histories. This capability\\nenables them to extract valuable insights that can be instrumental in credit and\\nrisk analysis. By analyzing historical data and market trends, LLMs can assist\\nanalysts in gaining a deeper understanding of market dynamics and individual\\ncredit risks. Nevertheless, it’s important to recognize that the effectiveness of\\nLLMs in credit scoring is still evolving. As the financial industry continues to\\nintegrate more advanced technological solutions, the role of LLMs in enhancing\\nthe accuracy and efficiency of credit and risk assessments will likely become\\nmore pronounced, heralding a new era in financial analytics.\\n3.3.2\\nESG Scoring\\nEnvironmental, Social, and Governance (ESG) scoring is a critical metric in the\\ncontemporary business and investment landscape. It serves as a tool for eval-\\nuating a company’s commitment to environmental stewardship, social respon-\\nsibility, and governance practices. ESG scores came about due to the financial\\nworld’s need to assess companies against these three criteria to identify the best\\nperformers in these aspects [23]. Private commercial firms whose primary clients\\nare portfolio managers and other investors use tangible and intangible data to\\nconstruct ESG scores to produce new data that meet investors’ needs [33].\\nA growing trend sees more companies being evaluated by sustainability rat-\\ning agencies. The objective of these assessments is to generate relevant data for\\nstakeholders interested in utilizing non-financial information about these com-\\n16\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5b167d59-1cbb-41de-af0f-d5f8b9605797', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '17'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='panies. The information is particularly valuable for those looking to assess their\\ninvestments or to develop investment portfolios based on sustainability criteria\\n[37]. There are several common approaches for ESG scoring. Firstly, compa-\\nnies like Refinitiv and Bloomberg collect data from public sources but do not\\noffer any value-adding input or scoring [146]. Secondly, ESG data providers\\ncombine both public and own-created data to evaluate ESG scores or ratings\\n(e.g. MSCI). Thirdly, some companies focus on specialized ESG issues such as\\nCarbon Disclosure Project.\\nThe integration of GPT-4 into the process of ESG scoring remains an abun-\\ndant blank area deserving to explore. Application of GPT-4 can offer numerous\\npotential benefits, enhancing both the efficiency and effectiveness of this increas-\\ningly important evaluation method. GPT-4 assists with enhanced data process-\\ning and analysis. Its advanced capabilities allow it to process vast amounts of\\nunstructured data rapidly including corporate sustainability reports, news arti-\\ncles, social media posts, and other relevant documents. By analyzing this data,\\nGPT-4 can extract key insights about a company’s ESG practices, providing\\na more comprehensive view than traditional methods. GPT-4 helps mitigate\\nhuman biases which leads to more objective and consistent ESG assessments.\\nIts ability to analyze data based on predefined criteria reduces subjective inter-\\npretation, improving the credibility of the scoring process. Besides, GPT-4 is\\nsuitable for real-time monitoring and dynamic scoring. GPT-4 can continuously\\nmonitor various data sources for real-time updates related to ESG factors. Dy-\\nnamic scoring is available to reflect the most current information, providing a\\nmore accurate and timely picture of a company’s ESG performance.\\nUtilizing GPT-4 for ESG scoring represents a significant advancement in\\nsustainability evaluation. Its ability makes it a potent tool for providing deeper,\\nmore accurate, and up-to-date insights into ESG performance.\\n3.3.3\\nFraud Detection\\nAs trade volume escalates and digital wallet technology advances, the realm of\\nfinancial risk management is increasingly facing sophisticated high-tech criminal\\nactivities. A case in point: during the initial nine months of 2023, over 83,000\\nAmericans fell prey to credit card fraud, leading to collective financial losses\\nfor the victims amounting to $183 million 1. Consequently, the implementation\\nof robust fraud detection applications is imperative to preserve the integrity\\nof financial systems and safeguard both the institutions and their clients from\\nfinancial losses [35]. Leveraging advanced reasoning and text mining capabil-\\nities, LLMs can significantly contribute to the identification of financial fraud\\nin various domains including transactions, emails, profiles, contractors, and de-\\ncentralized finance [84]. These LLMs serve as an initial filter, learning from\\ncustomer transaction histories and detailed transaction information to isolate\\nhighly suspicious transactions from the billions processed [106], thereby substan-\\ntially alleviating the manual labor burden involved in investigating vast quan-\\n1https://www.aura.com/learn/credit-card-scams\\n17\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f4d2bf08-2775-4a80-aba4-1f0f2999c548', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '18'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Figure 4: In compliance checks, due to rapid updates in regulation checklists,\\nmodels fine-tuned on outdated standards quickly become obsolete. Therefore,\\nwe increasingly rely on GPT-4’s zero-shot learning capabilities.\\ntities of transaction data. In this study, we use the PaySim simulates mobile\\nmoney transactions dataset [82] to evaluate GPT-4’s effectiveness in detecting\\nfraud.\\n3.3.4\\nCompliance Check\\nLLMs with zero-shot learning capabilities are becoming indispensable in the dy-\\nnamic world of financial compliance, where regulations are in a constant state\\nof flux. Zero-shot LLMs can adapt to new standards without the need for fine-\\ntuning, which traditionally demands regular updates and a wealth of annotated\\ndata [142]. This characteristic is particularly beneficial for tasks such as au-\\ndits, transaction monitoring, and reporting, as well as financial reporting and\\ndisclosure. In audits, zero-shot LLMs can immediately be deployed to parse\\nand analyze documents, identifying inconsistencies and irregularities by under-\\nstanding the underlying context, without the need for a model that is fine-tuned\\nto specific audit criteria which may change over time [67]. This saves valuable\\ntime and resources in an environment where regulatory frameworks can shift\\nunpredictably. For transaction monitoring and reporting, these LLMs excel at\\ndetecting anomalous patterns indicative of non-compliance or suspicious activ-\\nities. They are capable of understanding transactional nuances and alerting to\\nirregularities, all without prior fine-tuning to the specific rules that could be\\nsubject to change due to evolving regulations or market practices.\\nWhen it\\ncomes to financial reporting and information disclosure, zero-shot LLMs ensure\\nthat disclosures align with current reporting standards such as IFRS, even as\\nthose standards are updated. They provide an agile response to changing re-\\nquirements, highlighting discrepancies against the latest regulations without the\\nneed for retraining on new data sets. The zero-shot learning approach of LLMs\\nnot only bypasses the labor-intensive process of continuous model retraining but\\nalso mitigates the risk of outdated compliance checks in the rapidly changing\\nfinancial landscape. This agility makes zero-shot LLMs a critical tool for fi-\\nnancial institutions seeking to maintain compliance with the latest regulatory\\ndemands [23, 53].\\n18\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b708bd2a-f2c8-43e5-9349-a8940155568f', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '19'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='3.4\\nFinancial Real-Time Question Answering\\n3.4.1\\nFinancial Education\\nGPT-4 is an advanced artificial intelligence language model developed by Ope-\\nnAI that is capable of understanding and generating human-like natural lan-\\nguage [15, 25, 26, 40, 41, 43, 59, 63, 68, 72, 76, 77, 86, 111]. This feature makes\\nit a powerful tool for financial education, especially when it comes to explain-\\ning complex financial concepts, providing customized learning experiences, and\\nenhancing user interaction [125].\\nFirst, GPT-4 can simplify complex financial concepts into easy-to-understand\\nlanguage. The field of finance is full of complex terms and concepts such as\\nsecurities markets, portfolio diversification, risk management, etc. GPT-4 can\\nexplain these concepts more understandably through its deep learning and train-\\ning on large amounts of financial data. This is especially important for those\\nnew to finance because it lowers the learning curve, allowing them to more easily\\nunderstand and apply the concepts.\\nSecondly, GPT-4 has unique advantages in providing customized learning\\nexperience [29, 43, 64, 70, 74, 75, 78, 115]. It can adjust content and difficulty\\naccording to the user’s learning progress, interests and needs [40, 44, 62, 73, 111,\\n137, 143, 144]. For example, for beginners, GPT-4 can provide basic financial\\nknowledge and concepts; for more experienced learners, it can provide more in-\\ndepth analysis and advanced topics. This personalized learning approach helps\\nimprove learning efficiency and user satisfaction.\\nIn addition, GPT-4 plays an important role in enhancing user interaction.\\nThrough interactive Q&A, simulated scenarios, and real-time feedback, GPT-4\\ncan create a more dynamic and engaging learning environment. This method not\\nonly enhances the interest of learning but also helps improve learners’ practical\\nskills and problem-solving abilities.\\nHowever, although GPT-4 has many advantages in financial education, it\\nalso has some limitations. First, although GPT-4 is excellent at explaining fi-\\nnancial concepts and providing personalized teaching, it still relies on existing\\nknowledge bases and data. This means that GPT-4 may not be able to provide\\ncutting-edge information when faced with the latest financial trends and data.\\nFor example, in the context of rapidly changing financial markets, GPT-4 may\\nnot be able to update its knowledge base promptly to reflect the latest market\\ndynamics and regulatory changes. Secondly, GPT-4 also needs to consider eth-\\nical and compliance issues when providing financial education. The accuracy\\nand transparency of financial information are critical to protecting consumers\\nand maintaining market order.\\nTherefore, when using GPT-4 as a financial\\neducation tool, you must ensure that the information and advice it provides\\ncomply with relevant laws and regulations and are ethically responsible.\\nOverall, GPT-4 offers many potential advantages in the field of financial ed-\\nucation, including simplifying complex concepts, providing a personalized learn-\\ning experience, and enhancing user interaction. However, its application also\\nneeds to take into account challenges such as accuracy, timeliness, and ethical\\n19\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='a36a42f3-5aa0-49fd-baa4-3fdc152f5dc6', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '20'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='compliance. In the future, with the continuous development and improvement\\nof technology, GPT-4 has the potential to become an important auxiliary tool\\nin the field of financial education, helping users to better understand and apply\\nfinancial knowledge.\\n4\\nGPT-4 Empowered Financial Tasks Evalua-\\ntions\\nIn this section, we introduce the approach used in our survey to evaluate the\\nperformance of GPT-4 in a variety of financial tasks with one-shot learning and\\nzero-shot prompting. Our method consists of several important parts, including\\npractical financial tasks, selection of benchmark data sets, design of various\\ninstruction prompts, and selection of evaluation indicators.\\n4.1\\nDatasets\\nTo showcase the extensive capabilities of GPT-4 in the financial sector, we have\\nmeticulously chosen six diverse datasets. These datasets encompass a wide range\\nof text types, including news articles, analytical reports, and social media posts\\nlike tweets. In addition, we’ve incorporated featuring time series, tabular data,\\nand textual content. Furthermore, we’ve crafted a series of comprehensive and\\npractical financial tasks that mirror real-world finance scenarios. This selection\\nand design aim to fully demonstrate the advanced and versatile applications of\\nthese AI models in finance.\\nTable 1: The details of the raw data and instruction data.\\nData\\nTask\\nRaw\\nInstruction\\nData Types\\nModalities\\nLicense\\nFPB\\nsentiment analysis\\n4,845\\n48,450\\nnews\\ntext\\nCC BY-SA 3.0\\nFiQA-SA\\nsentiment analysis\\n1,173\\n11,730\\nnews headlines,tweets\\ntext\\nPublic\\nNER\\nnamed entity recognition\\n1,366\\n13,660\\nfinancial agreements\\ntext\\nCC BY-SA 3.0\\nFinQA\\nquestion answering\\n8,281\\n8,281\\nearnings reports\\ntext,table\\nMIT License\\nConvFinQA\\nquestion answering\\n3,892\\n3,892\\nearnings reports\\ntext,table\\nMIT License\\nBigData22\\nstock movement prediction\\n7,164\\n7,164\\ntweets,historical prices\\ntext,time series\\nPublic\\nEvaluating Sentiment in Financial News: The task of discerning sen-\\ntiment in financial news is a paramount concern within the financial analytics\\ncommunity, as underscored in seminal works by Araci\\n[5] and Yang et al.\\n[131]. This endeavor seeks to meticulously interpret the sentiment embedded\\nin financial narratives. Adhering to the established FLUE framework\\n[110],\\nthis study employs two prominent datasets: the Financial Phrase Bank (FPB)\\ndataset\\n[89] and FiQA-SA\\n[87].\\nThe FPB dataset is a collection of finan-\\ncial news excerpts, each meticulously annotated by field experts with a senti-\\nment classification: positive, negative, or neutral. Conversely, FiQA-SA serves\\nas an expansive dataset predominantly utilized for the sentiment quantifica-\\ntion of English-language financial reporting and microblogging content, using\\na nuanced sentiment intensity scale ranging from -1 to 1, where a value of 1\\nepitomizes the most positive sentiment.\\n20\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='16ac32c0-129a-4e57-8337-e1eb54d876bf', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '21'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Figure 5: We conducted sentiment analysis on 970 data points from the FiQA-\\nSA task set[128]. By using GPT-4, we achieve 79$ accuracy rate.\\nFigure 6: GPT-4 has demonstrated its zero-shot learning and instruction fol-\\nlowing capacities.\\nIdentifying Named Entities in Finance: The goal of this task is to pin-\\npoint key financial entities, including individuals, organizations, and locations.\\nThese entities are crucial for developing financial knowledge graphs. The NER\\ndataset\\n[4] serves as the basis for this task, featuring sentences from finan-\\ncial agreements filed with the U.S. Securities and Exchange Commission and\\nincludes entities categorized as LOCATION, ORGANISATION, and PERSON.\\nFinancial Question Answering: This task involves automatically re-\\nsponding to financial queries based on provided data. For this, two datasets are\\nemployed: FinQA [20] and ConvFinQA [21]. FinQA offers pairs of questions\\nand answers, annotated by specialists, along with associated earnings reports\\nfrom S&P 500 companies. ConvFinQA extends this by including multi-turn\\ndialogues over these earnings reports.\\nPredicting Stock Movements: Recognized as a critical financial task,\\npredicting stock movements can be invaluable in practical applications like in-\\nvestment strategy formulation. This task is approached as a binary classification\\nchallenge, following the methodology of prior research [112]. It involves fore-\\ncasting the direction of stock price movements based on historical prices and\\n21\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='adf4dca1-c59e-4be0-96be-3e9f91e9b0b8', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '22'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Figure 7: Using GPT-4 for summary demonstrates LLMs’ knowledge integra-\\ntion capabilities, logical reasoning capabilities and language expression capabil-\\nities.Green highlight indicates the k\\nrelevant tweets. Movements above 0.55% are considered positive, while those\\nbelow -0.5% are deemed negative. For this analysis, one widely used datasets is\\nutilized: BigData22 [112].\\n4.2\\nPrompt Design\\nWe examined various prompting strategies, including vanilla zero-shot prompt-\\ning, Chain-of Thought (CoT) enhanced zero-shot prompting, and one-shot prompt-\\ning to investigate their impact on GPT’s performance in the stated financial\\ntasks. The formulation of prompts is essential in interacting with LLMs. An\\nelaborate and well-organized prompt, detailed and clear, leads to outputs that\\nare more accurate and in line with the provided instructions. The following\\nthree parts are the GPT4-prompt in the financial field that we have obtained\\nthrough experiments to generate output that best meets the instructions.\\n• System Role Explanation: This section will describe the specific role\\nand tasks GPT-4 is expected to perform in a financial setting. For exam-\\nple, it might be tasked with analyzing market trends, offering investment\\nadvice, or interpreting financial reports.\\n• Response Format for Different Tasks: This section has specific re-\\nquirements for the format of the output.\\nFor example, you want the\\ninformation to be presented in the form of lists, charts, or reports.\\n• Example and Output: This part would provide a example as a guideline\\nfor a finance-related query. Also, it contains the desired response for the\\nquery.\\n22\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='b4687d69-bd2d-4ef9-82bf-9d464d9c19c5', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '23'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Figure 8: We conducted stock price prediction on 1,470 data points from the\\nBigData task set[128]. By using GPT-4, we achieve 51$ accuracy rate.\\nThe first three components are utilized for system message input in response\\nto each query. We have attempted to enhance the precision of the prompts by\\nincorporating additional components, yet this has not resulted in a substantial\\nimprovement in performance.\\n5\\nExperimental Results\\nIn our tested financial tasks, LLMs demonstrated precise execution capabili-\\nties. Based on the responses we gathered, we believe that LLMs exhibit excep-\\ntional zero-shot learning and mathematical reasoning abilities, along with their\\nstrongest suit, language sentiment analysis. The effectiveness of LLMs in finan-\\ncial tasks is quantitatively assessed by comparing their recommendations against\\nreal-world financial data and historical market performance. This methodology\\nwas rigorously tested across various financial scenarios and datasets, yielding in-\\nsightful and actionable results in areas such as financial engineering, risk assess-\\nment, and market trend analysis. For financial tasks lacking dedicated datasets,\\nwe have curated case studies to showcase the capabilities of GPT-4.\\nTable 2: The zero-shot and few-shot performance of different LLMs on the\\nstated datasets.\\nDataset\\nMetrics\\nGPT 4\\nFPB\\nAcc\\n0.78\\nFiQA-SA\\nAcc\\n0.79\\nNER\\nEntityF1\\n0.81\\nFinQA\\nEmAcc\\n0.64\\nConvFinQA\\nEmAcc\\n0.73\\nBigData22\\nAcc\\n0.53\\n23\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4b022927-aef2-4115-8160-7b9242ae5a16', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '24'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text=\"USER: You are a helpful financial fraud assistant. Your job is to classify the transaction as \\nsuspicious/fraudulent or not. \\n5 transaction data as shown in the image, Right is the column header's meaning:\\nstep: Maps a unit of time in the real world. In this case 1 \\nstep is 1 hour of time.\\ntype: CASH-IN, CASH-OUT, DEBIT, PAYMENT and \\nTRANSFER\\namount: amount of the transaction in local currency\\nnameOrig: customer who started the transaction\\noldbalanceOrg: initial balance before the transaction\\nnewbalanceOrig: customer's balance after the \\ntransaction.\\nnameDest: recipient ID of the transaction.\\noldbalanceDest: initial recipient balance before the \\ntransaction.\\nnewbalanceDest: recipient's balance after the \\ntransaction.\\nGPT-4: Based on the provided information and the image of transaction data, I will analyze each transaction for potential signs of fraud.\\n• Transaction 0: PAYMENT type transactions are generally less suspicious, but we should consider the fact that the destination balance does not change, which \\ncould indicate a merchant or bill payment where balance is not typically reflected.\\n• Transaction 1: Like transaction 0, this is a PAYMENT, and there's no change in the destination balance, which is consistent with a payment to a merchant or a bill.\\n• Transaction 2: TRANSFER transactions are inherently riskier due to the ease with which funds can be moved. However, the transaction amount exactly matches the \\nold balance, which is a bit unusual. Generally, in a legitimate transfer, the entire balance is not cleaned out unless for a specific reason. This warrants a closer look.\\n• Transaction 3: This CASH_OUT transaction is particularly suspicious. The old balance of the destination matches the transaction amount, and the new balance is \\nzero. This could indicate that the funds were taken out of the destination account immediately after being transferred, a pattern that could be consistent with \\nmoney laundering or cash-out scams.\\n• Transaction 4: This is a PAYMENT, and the amount subtracted from the original balance corresponds to the transaction amount. As with the other payments, the \\nlack of change in the destination balance is not unusual for this type of transaction.\\nIn summary, transactions 0, 1, and 4 seem typical of payment transactions. Transaction 2 might require additional verification due to the full balance being \\ntransferred, which can sometimes be a red flag. Transaction 3 is the most suspicious and should be flagged for further investigation due to the patterns observed \\nwhich are commonly associated with fraudulent activity.\\nFigure 9: Fraud detection on PaySim simulates mobile money transactions\\ndataset [82] using GPT-4: 5 out of 5 Correct. Green highlight indicates normal\\ntransactions; Yellow highlight indicates suspicious/fraudulent transactions.\\n6\\nLimitation and Future work\\nThe limitations of LLMs are evident in areas such as optimization and quanti-\\ntative trading. While they can assist in identifying market sentiments, LLMs\\ncannot directly engage in computational tasks. Their role is more auxiliary,\\naiding in sentiment analysis which then feeds into existing models that handle\\nquantitative variables [123]. This indicates that LLMs, as of now, are not stan-\\ndalone solutions for computational finance tasks but rather powerful tools for\\naugmenting existing models.\\nFor future work, there is immense potential in integrating LLMs with ad-\\nvanced quantitative models. One promising direction could be the development\\nof hybrid systems that combine the text processing prowess of LLMs with so-\\nphisticated quantitative trading algorithms [22, 135]. Another area could be\\nenhancing the interpretability and reliability of LLMs outputs in financial con-\\ntexts, ensuring that the insights generated are not only accurate but also ac-\\ntionable. Moreover, exploring the application of LLMs in predictive analytics\\nfor market trends, based on historical data and current events, can open new\\navenues in financial forecasting. This integration of qualitative and quantitative\\nanalysis could revolutionize how financial markets are analyzed and traded [39].\\n7\\nConclusion\\nIn this article, we have delved into the multifaceted application of GPT-4 across\\na spectrum of 11 financial tasks, shedding light on the capabilities and con-\\n24\\n\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c8c3c6d3-a506-4e3e-b568-907894900d7d', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '25'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='straints of LLMs in the financial domain.\\nCentral to our findings is the re-\\nmarkable adeptness of LLMs in text processing, sentiment analysis, and their\\nzero-shot learning abilities. The proficiency of LLMs in sifting through and in-\\nterpreting extensive textual data is unmatched, thus playing a pivotal role in\\ndecoding market dynamics and investor sentiment.\\nHowever, it is crucial to acknowledge the limitations of LLMs in direct com-\\nputational tasks, particularly in optimization and quantitative trading, where\\ntheir role remains largely supplementary. Despite these constraints, the poten-\\ntial of LLMs in enhancing financial models and decision-making processes is\\nundeniable. As we advance, the integration of LLMs with quantitative models\\nand the refinement of their application in finance will be areas of significant\\ninterest. The continual evolution of LLMs promises to not only bolster existing\\nfinancial methodologies but also to pave the way for innovative approaches in\\nfinancial analysis and strategy.\\nReferences\\n[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L.,\\nAlmeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4\\ntechnical report. arXiv preprint arXiv:2303.08774 (2023)\\n[2] Ahmadi, Z., Martens, P., Koch, C., Gottron, T., Kramer, S.: Towards\\nbankruptcy prediction: Deep sentiment mining to detect financial distress\\nfrom business management reports. In: 2018 IEEE 5th International Con-\\nference on Data Science and Advanced Analytics (DSAA). pp. 293–302.\\nIEEE (2018)\\n[3] Ali, S.R.M.: Geopolitical threat, market capitalization, and portfolio re-\\nturn. Market Capitalization, and Portfolio Return\\n[4] Alvarado, J.C.S., Verspoor, K., Baldwin, T.: Domain adaption of named\\nentity recognition to support credit risk assessment. In: Proceedings of\\nthe Australasian Language Technology Association Workshop 2015. pp.\\n84–90 (2015)\\n[5] Araci, D.: Finbert: Financial sentiment analysis with pre-trained lan-\\nguage models. arXiv preprint arXiv:1908.10063 (2019)\\n[6] Audrino, F., Sigrist, F., Ballinari, D.: The impact of sentiment and at-\\ntention measures on stock market volatility. International Journal of Fore-\\ncasting 36(2), 334–357 (2020)\\n[7] Aw, E.C.X., Leong, L.Y., Hew, J.J., Rana, N.P., Tan, T.M., Jee, T.W.:\\nCounteracting dark sides of robo-advisors: justice, privacy and intrusion\\nconsiderations. International Journal of Bank Marketing (2023)\\n[8] Babaei, G., Giudici, P.: Gpt classifications, with application to credit\\nlending. Available at SSRN 4649285 (2023)\\n25\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d06d07f8-04f1-4fea-8f37-4123014bf72b', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '26'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[9] Baker, T., Dellaert, B.: Regulating robo advice across the financial ser-\\nvices industry. Iowa L. Rev. 103, 713 (2017)\\n[10] Bollen, J., Mao, H., Zeng, X.: Twitter mood predicts the stock market.\\nJournal of computational science 2(1), 1–8 (2011)\\n[11] Brigo, D., Mercurio, F.: Interest rate models-theory and practice: with\\nsmile, inflation and credit, vol. 2. Springer (2006)\\n[12] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal,\\nP., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language\\nmodels are few-shot learners. Advances in neural information processing\\nsystems 33, 1877–1901 (2020)\\n[13] Buehler, H., Gonon, L., Teichmann, J., Wood, B.: Deep hedging. Quan-\\ntitative Finance 19(8), 1271–1291 (2019)\\n[14] Cai, H., Liao, W., Liu, Z., Zhang, Y., Huang, X., Ding, S., Ren, H., Wu,\\nZ., Dai, H., Li, S., Wu, L., Liu, N., Li, Q., Liu, T., Li, X.: Coarse-to-\\nfine knowledge graph domain adaptation based on distantly-supervised\\niterative training (2023)\\n[15] Cai, H., Liao, W., Liu, Z., Zhang, Y., Huang, X., Ding, S., Ren, H.,\\nWu, Z., Dai, H., Li, S., et al.: Coarse-to-fine knowledge graph domain\\nadaptation based on distantly-supervised iterative training. arXiv preprint\\narXiv:2211.02849 (2022)\\n[16] Caspi, I., Felber, S.S., Gillis, T.B.: Generative ai and the future of finan-\\ncial advice regulation\\n[17] Chang,\\nC.,\\nPeng,\\nW.C.,\\nChen,\\nT.F.:\\nLlm4ts:\\nTwo-stage fine-\\ntuning for time-series forecasting with pre-trained llms. arXiv preprint\\narXiv:2308.08469 (2023)\\n[18] Chang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., Yang, L., Yi,\\nX., Wang, C., Wang, Y., et al.: A survey on evaluation of large language\\nmodels. arXiv preprint arXiv:2307.03109 (2023)\\n[19] Chen, J.M.: Models for predicting business bankruptcies and their appli-\\ncation to banking and financial regulation. Penn St. L. Rev. 123,\\n735\\n(2018)\\n[20] Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa,\\nR., Beane, M., Huang, T.H., Routledge, B., et al.: Finqa: A dataset of\\nnumerical reasoning over financial data. arXiv preprint arXiv:2109.00122\\n(2021)\\n[21] Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., Wang, W.Y.: Convfinqa:\\nExploring the chain of numerical reasoning in conversational finance ques-\\ntion answering. arXiv preprint arXiv:2210.03849 (2022)\\n26\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='710a4869-f3fb-49fd-8a5b-d083a0d8c11f', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '27'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[22] Ciuriak, D., Artyushina, A.: Trading ai: Machine knowledge capital and\\nthe trading system. Available at SSRN (2023)\\n[23] Cl´ement, A., Robinot, ´E., Trespeuch, L.: Improving esg scores with sus-\\ntainability concepts. Sustainability 14(20), 13154 (2022)\\n[24] Cont, R.: Empirical properties of asset returns: stylized facts and statis-\\ntical issues. Quantitative finance 1(2), 223 (2001)\\n[25] Dai, H., Li, Y., Liu, Z., Zhao, L., Wu, Z., Song, S., Shen, Y., Zhu, D., Li,\\nX., Li, S., Yao, X., Shi, L., Li, Q., Chen, Z., Zhang, D., Mai, G., Liu, T.:\\nAd-autogpt: An autonomous gpt for alzheimer’s disease infodemiology\\n(2023)\\n[26] Dai, H., Liu, Z., Liao, W., Huang, X., Cao, Y., Wu, Z., Zhao, L., Xu,\\nS., Liu, W., Liu, N., Li, S., Zhu, D., Cai, H., Sun, L., Li, Q., Shen, D.,\\nLiu, T., Li, X.: Auggpt: Leveraging chatgpt for text data augmentation\\n(2023)\\n[27] Dastile, X., Celik, T., Potsane, M.: Statistical and machine learning mod-\\nels in credit scoring: A systematic literature survey. Applied Soft Com-\\nputing 91, 106263 (2020)\\n[28] Deng, X., Bashlovkina, V., Han, F., Baumgartner, S., Bendersky, M.:\\nWhat do llms know about financial markets?\\na case study on reddit\\nmarket sentiment analysis. In: Companion Proceedings of the ACM Web\\nConference 2023. pp. 107–110 (2023)\\n[29] Dou, F., Ye, J., Yuan, G., Lu, Q., Niu, W., Sun, H., Guan, L., Lu, G.,\\nMai, G., Liu, N., Lu, J., Liu, Z., Wu, Z., Tan, C., Xu, S., Wang, X., Li,\\nG., Chai, L., Li, S., Sun, J., Sun, H., Shao, Y., Li, C., Liu, T., Song, W.:\\nTowards artificial general intelligence (agi) in the internet of things (iot):\\nOpportunities and challenges (2023)\\n[30] Ehrmann, M., Hamdi, A., Pontes, E.L., Romanello, M., Doucet, A.:\\nNamed entity recognition and classification on historical documents: A\\nsurvey (2021)\\n[31] Ehrmann, M., Hamdi, A., Pontes, E.L., Romanello, M., Doucet, A.:\\nNamed entity recognition and classification in historical documents: A\\nsurvey. ACM Computing Surveys 56(2), 1–47 (2023)\\n[32] Ekbal, A., Bandyopadhyay, S.: Named entity recognition using support\\nvector machine: A language independent approach. International Journal\\nof Electrical and Computer Engineering 4(3), 589–604 (2010)\\n[33] Escrig-Olmedo,\\nE.,\\nFern´andez-Izquierdo,\\nM.´A.,\\nFerrero-Ferrero,\\nI.,\\nRivera-Lirio, J.M., Mu˜noz-Torres, M.J.: Rating the raters: Evaluating\\nhow esg rating agencies integrate sustainability principles. Sustainability\\n11(3), 915 (2019)\\n27\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dc22e306-0ba4-4c27-8c4e-e0cc7f272565', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '28'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[34] Fan, J.: A selective overview of nonparametric methods in financial econo-\\nmetrics. Statistical Science pp. 317–337 (2005)\\n[35] Feng, D., Dai, Y., Huang, J., Zhang, Y., Xie, Q., Han, W., Lopez-Lira,\\nA., Wang, H.: Empowering many, biasing a few: Generalist credit scoring\\nthrough large language models. arXiv preprint arXiv:2310.00566 (2023)\\n[36] Freitag, M., Al-Onaizan, Y.: Beam search strategies for neural machine\\ntranslation. arXiv preprint arXiv:1702.01806 (2017)\\n[37] Friede, G., Busch, T., Bassen, A.: Esg and financial performance: aggre-\\ngated evidence from more than 2000 empirical studies. Journal of sustain-\\nable finance & investment 5(4), 210–233 (2015)\\n[38] Gage, P.: A new algorithm for data compression. C Users Journal 12(2),\\n23–38 (1994)\\n[39] Ge, Y., Hua, W., Ji, J., Tan, J., Xu, S., Zhang, Y.: Openagi: When llm\\nmeets domain experts. arXiv preprint arXiv:2304.04370 (2023)\\n[40] Gong, X., Holmes, J., Li, Y., Liu, Z., Gan, Q., Wu, Z., Zhang, J., Zou, Y.,\\nTeng, Y., Jiang, T., Zhu, H., Liu, W., Liu, T., Yan, Y.: Evaluating the\\npotential of leading large language models in reasoning biology questions\\n(2023)\\n[41] Guan, Z., Wu, Z., Liu, Z., Wu, D., Ren, H., Li, Q., Li, X., Liu, N.:\\nCohortgpt: An enhanced gpt for participant recruitment in clinical study\\n(2023)\\n[42] Gupta, U.: Gpt-investar: Enhancing stock investment strategies through\\nannual report analysis with large language models. arXiv preprint\\narXiv:2309.03079 (2023)\\n[43] Holmes, J., Liu, Z., Zhang, L., Ding, Y., Sio, T., McGee, L., Ashman, J.,\\nLi, X., Liu, T., Shen, J., et al.: Evaluating large language models on a\\nhighly-specialized topic. Radiation Oncology Physics (2023)\\n[44] Holmes, J., Zhang, L., Ding, Y., Feng, H., Liu, Z., Liu, T., Wong, W.W.,\\nVora, S.A., Ashman, J.B., Liu, W.: Benchmarking a foundation llm on\\nits ability to re-label structure names in accordance with the aapm tg-263\\nreport (2023)\\n[45] Jeong, C.: Fine-tuning and utilization methods of domain-specific llms\\n(2024)\\n[46] Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J.Y., Shi, X., Chen, P.Y.,\\nLiang, Y., Li, Y.F., Pan, S., et al.: Time-llm: Time series forecasting by\\nreprogramming large language models. arXiv preprint arXiv:2310.01728\\n(2023)\\n28\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c6ddccf7-6e58-4147-ba9e-3604659a2377', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '29'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[47] Johansen, S.: Likelihood-based inference in cointegrated vector autore-\\ngressive models. OUP Oxford (1995)\\n[48] Kalra, S., Prasad, J.S.: Efficacy of news sentiment for stock market predic-\\ntion. In: 2019 International Conference on Machine Learning, Big Data,\\nCloud and Parallel Computing (COMITCon). pp. 491–496. IEEE (2019)\\n[49] Kalyanaraman, V., Kazi, S., Tondulkar, R., Oswal, S.: Sentiment analysis\\non news articles for stocks. In: 2014 8th Asia Modelling Symposium. pp.\\n10–15. IEEE (2014)\\n[50] Khedr, A.E., Yaseen, N., et al.: Predicting stock market behavior using\\ndata mining technique and news sentiment analysis. International Journal\\nof Intelligent Systems and Applications 9(7), 22 (2017)\\n[51] Kim, A.G., Yoon, S.: Corporate bankruptcy prediction with domain-\\nadapted bert. In: EMNLP 2021, 3rd Workshop on ECONLP (2021)\\n[52] Kirange, D., Deshmukh, R.R., et al.: Sentiment analysis of news head-\\nlines for stock price prediction. Composoft, An International Journal of\\nAdvanced Computer Technology 5(3), 2080–2084 (2016)\\n[53] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language\\nmodels are zero-shot reasoners. Advances in neural information processing\\nsystems 35, 22199–22213 (2022)\\n[54] Lee, C.F., Chen, H.Y., Lee, J.: Financial econometrics, mathematics and\\nstatistics. Springer (2019)\\n[55] Lee, G.G., Shi, L., Latif, E., Gao, Y., Bewersdorf, A., Nyaaba, M., Guo,\\nS., Wu, Z., Liu, Z., Wang, H., et al.: Multimodality of ai for education:\\nTowards artificial general intelligence. arXiv preprint arXiv:2312.06037\\n(2023)\\n[56] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,\\nStoyanov, V., Zettlemoyer, L.: Bart: Denoising sequence-to-sequence pre-\\ntraining for natural language generation, translation, and comprehension.\\narXiv preprint arXiv:1910.13461 (2019)\\n[57] Li, J., Sun, A., Han, J., Li, C.: A survey on deep learning for named en-\\ntity recognition. IEEE Transactions on Knowledge and Data Engineering\\n34(1), 50–70 (2020)\\n[58] Li, Q., Wang, T., Li, P., Liu, L., Gong, Q., Chen, Y.: The effect of news\\nand public mood on stock movements. Information Sciences 278, 826–840\\n(2014)\\n[59] Li, X., Zhang, L., Wu, Z., Liu, Z., Zhao, L., Yuan, Y., Liu, J., Li, G.,\\nZhu, D., Yan, P., Li, Q., Liu, W., Liu, T., Shen, D.: Artificial general\\nintelligence for medical imaging (2023)\\n29\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='00d1ad4a-baae-426f-b1ad-8678820c89bb', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '30'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[60] Li, X., Wu, P., Wang, W.: Incorporating stock prices and news sentiments\\nfor stock market prediction: A case of hong kong. Information Processing\\n& Management 57(5), 102212 (2020)\\n[61] Liao, W., Liu, Z., Dai, H., Wu, Z., Zhang, Y., Huang, X., Chen, Y., Jiang,\\nX., Liu, W., Zhu, D., Liu, T., Li, S., Li, X., Cai, H.: Mask-guided bert\\nfor few shot text classification (2023)\\n[62] Liao, W., Liu, Z., Dai, H., Wu, Z., Zhang, Y., Huang, X., Chen, Y.,\\nJiang, X., Zhu, D., Liu, T., et al.: Mask-guided bert for few shot text\\nclassification. arXiv preprint arXiv:2302.10447 (2023)\\n[63] Liao, W., Liu, Z., Dai, H., Xu, S., Wu, Z., Zhang, Y., Huang, X., Zhu,\\nD., Cai, H., Liu, T., Li, X.: Differentiate chatgpt-generated and human-\\nwritten medical texts (2023)\\n[64] Liu,\\nC.,\\nLiu,\\nZ.,\\nHolmes,\\nJ.,\\nZhang,\\nL.,\\nZhang,\\nL.,\\nDing,\\nY.,\\nShu,\\nP.,\\nWu,\\nZ.,\\nDai,\\nH.,\\nLi,\\nY.,\\nShen,\\nD.,\\nLiu,\\nN.,\\nLi,\\nQ.,\\nLi,\\nX.,\\nZhu,\\nD.,\\nLiu,\\nT.,\\nLiu,\\nW.:\\nArtificial\\ngeneral\\nin-\\ntelligence for radiation oncology. Meta-Radiology p. 100045 (2023).\\nhttps://doi.org/https://doi.org/10.1016/j.metrad.2023.100045, https://\\nwww.sciencedirect.com/science/article/pii/S2950162823000450\\n[65] Liu, C., Liu, Z., Holmes, J., Zhang, L., Zhang, L., Ding, Y., Shu, P.,\\nWu, Z., Dai, H., Li, Y., et al.: Artificial general intelligence for radiation\\noncology. Meta-Radiology p. 100045 (2023)\\n[66] Liu, X.Y., Wang, G., Yang, H., Zha, D.: Fingpt: Democratizing internet-\\nscale data for financial large language models (2023)\\n[67] Liu, X., Li, H., Zhu, X.: A gpt-based method of automated compliance\\nchecking through prompt engineering (2023)\\n[68] Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He,\\nM., Liu, Z., et al.: Summary of chatgpt-related research and perspective\\ntowards the future of large language models. Meta-Radiology p. 100017\\n(2023)\\n[69] Liu, Y., He, H., Han, T., Zhang, X., Liu, M., Tian, J., Zhang, Y., Wang, J.,\\nGao, X., Zhong, T., et al.: Understanding llms: A comprehensive overview\\nfrom training to inference. arXiv preprint arXiv:2401.02038 (2024)\\n[70] Liu, Z., Zhong, A., Li, Y., Yang, L., Ju, C., Wu, Z., et al.: Radiology-gpt:\\na large language model for radiology. arxiv [preprint]. 2023 [cited august\\n21, 2023]\\n[71] Liu, Z., He, M., Jiang, Z., Wu, Z., Dai, H., Zhang, L., Luo, S., Han,\\nT., Li, X., Jiang, X., et al.: Survey on natural language processing in\\nmedical image analysis. Zhong nan da xue xue bao. Yi xue ban= Journal\\nof Central South University. Medical Sciences 47(8), 981–993 (2022)\\n30\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4bdc2bb1-3c2a-4733-a78d-98ffed56e5e9', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '31'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[72] Liu, Z., He, X., Liu, L., Liu, T., Zhai, X.: Context matters: A strat-\\negy to pre-train language model for science education. arXiv preprint\\narXiv:2301.12031 (2023)\\n[73] Liu, Z., Li, Y., Cao, Q., Chen, J., Yang, T., Wu, Z., Hale, J., Gibbs, J.,\\nRasheed, K., Liu, N., et al.: Transformation vs tradition: Artificial general\\nintelligence (agi) for arts and humanities. arXiv preprint arXiv:2310.19626\\n(2023)\\n[74] Liu, Z., Li, Y., Shu, P., Zhong, A., Yang, L., Ju, C., Wu, Z., Ma, C., Luo,\\nJ., Chen, C., Kim, S., Hu, J., Dai, H., Zhao, L., Zhu, D., Liu, J., Liu, W.,\\nShen, D., Liu, T., Li, Q., Li, X.: Radiology-llama2: Best-in-class large\\nlanguage model for radiology (2023)\\n[75] Liu, Z., Wang, P., Li, Y., Holmes, J., Shu, P., Zhang, L., Liu, C., Liu, N.,\\nZhu, D., Li, X., Li, Q., Patel, S.H., Sio, T.T., Liu, T., Liu, W.: Radonc-\\ngpt: A large language model for radiation oncology (2023)\\n[76] Liu, Z., Wu, Z., Hu, M., Zhao, B., Zhao, L., Zhang, T., Dai, H., Chen,\\nX., Shen, Y., Li, S., Murray, B., Liu, T., Sikora, A.: Pharmacygpt: The\\nai pharmacist (2023)\\n[77] Liu, Z., Yu, X., Zhang, L., Wu, Z., Cao, C., Dai, H., Zhao, L., Liu, W.,\\nShen, D., Li, Q., Liu, T., Zhu, D., Li, X.: Deid-gpt: Zero-shot medical\\ntext de-identification by gpt-4 (2023)\\n[78] Liu, Z., Zhong, A., Li, Y., Yang, L., Ju, C., Wu, Z., Ma, C., Shu, P.,\\nChen, C., Kim, S., Dai, H., Zhao, L., Zhu, D., Liu, J., Liu, W., Shen, D.,\\nLi, Q., Liu, T., Li, X.: Tailoring large language models to radiology: A\\npreliminary approach to llm adaptation for a highly specialized domain.\\nIn: Cao, X., Xu, X., Rekik, I., Cui, Z., Ouyang, X. (eds.) Machine Learn-\\ning in Medical Imaging. pp. 464–473. Springer Nature Switzerland, Cham\\n(2024)\\n[79] Liu, Z., Zhong, A., Li, Y., Yang, L., Ju, C., Wu, Z., Ma, C., Shu, P., Chen,\\nC., Kim, S., Dai, H., Zhao, L., Zhu, D., Liu, J., Liu, W., Shen, D., Li,\\nX., Li, Q., Liu, T.: Radiology-gpt: A large language model for radiology\\n(2023)\\n[80] Liu, Z., Zhong, A., Li, Y., Yang, L., Ju, C., Wu, Z., Ma, C., Shu, P., Chen,\\nC., Kim, S., et al.: Radiology-gpt: A large language model for radiology.\\narXiv preprint arXiv:2306.08666 (2023)\\n[81] Lopez-Lira, A., Tang, Y.:\\nCan chatgpt forecast stock price move-\\nments? return predictability and large language models. arXiv preprint\\narXiv:2304.07619 (2023)\\n[82] Lopez-Rojas, E., Elmir, A., Axelsson, S.: Paysim: A financial mobile\\nmoney simulator for fraud detection. In: 28th European Modeling and\\n31\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2747f2f3-f220-47aa-bcb0-decd034ecf07', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '32'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='Simulation Symposium, EMSS, Larnaca. pp. 249–255. Dime University of\\nGenoa (2016)\\n[83] Louren¸co, C.J., Dellaert, B.G., Donkers, B.: Whose algorithm says so:\\nThe relationships between type of firm, perceptions of trust and exper-\\ntise, and the acceptance of financial robo-advice. Journal of Interactive\\nMarketing 49, 107–124 (2020)\\n[84] Luo, B., Zhang, Z., Wang, Q., Ke, A., Lu, S., He, B.: Ai-powered fraud\\ndetection in decentralized finance: A project life cycle perspective. arXiv\\npreprint arXiv:2308.15992 (2023)\\n[85] Luo, L., Yang, Z., Yang, P., Zhang, Y., Wang, L., Lin, H., Wang, J.: An\\nattention-based bilstm-crf approach to document-level chemical named\\nentity recognition. Bioinformatics 34(8), 1381–1388 (2018)\\n[86] Ma, C., Wu, Z., Wang, J., Xu, S., Wei, Y., Liu, Z., Jiang, X., Guo,\\nL., Cai, X., Zhang, S., Zhang, T., Zhu, D., Shen, D., Liu, T., Li, X.:\\nImpressiongpt: An iterative optimizing framework for radiology report\\nsummarization with chatgpt (2023)\\n[87] Maia, M., Handschuh, S., Freitas, A., Davis, B., McDermott, R., Zarrouk,\\nM., Balahur, A.: Www’18 open challenge: financial opinion mining and\\nquestion answering. In: Companion proceedings of the the web conference\\n2018. pp. 1941–1942 (2018)\\n[88] Makridakis, S., Petropoulos, F., Kang, Y.: Large language models: Their\\nsuccess and impact. Forecasting 5(3), 536–549 (2023)\\n[89] Malo, P., Sinha, A., Korhonen, P., Wallenius, J., Takala, P.: Good debt\\nor bad debt: Detecting semantic orientations in economic texts. Journal\\nof the Association for Information Science and Technology 65(4), 782–796\\n(2014)\\n[90] Maple, C., Szpruch, L., Epiphaniou, G., Staykova, K., Singh, S., Penwar-\\nden, W., Wen, Y., Wang, Z., Hariharan, J., Avramovic, P.: The ai revo-\\nlution: opportunities and challenges for the finance sector. arXiv preprint\\narXiv:2308.16538 (2023)\\n[91] Mittal,\\nA.,\\nGoel,\\nA.:\\nStock\\nprediction\\nusing\\ntwit-\\nter\\nsentiment\\nanalysis.\\nStandford\\nUniversity,\\nCS229\\n(2011\\nhttp://cs229.\\nstanford.\\nedu/proj2011/GoelMittal-\\nStockMarketPredictionUsingTwitterSentimentAnalysis. pdf) 15,\\n2352\\n(2012)\\n[92] Mohan, S., Mullapudi, S., Sammeta, S., Vijayvergia, P., Anastasiu, D.C.:\\nStock price prediction using news sentiment analysis. In: 2019 IEEE fifth\\ninternational conference on big data computing service and applications\\n(BigDataService). pp. 205–208. IEEE (2019)\\n32\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='9b8c1528-937a-4b13-b9e5-3d7d59b36074', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '33'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[93] Nasar, Z., Jaffry, S.W., Malik, M.K.:\\nNamed entity recognition and\\nrelation extraction: State-of-the-art. ACM Computing Surveys (CSUR)\\n54(1), 1–39 (2021)\\n[94] Nelson, B.K.: Time series analysis using autoregressive integrated mov-\\ning average (arima) models. Academic emergency medicine 5(7), 739–744\\n(1998)\\n[95] Nguyen, T.H., Shirai, K., Velcin, J.: Sentiment analysis on social media\\nfor stock movement prediction. Expert Systems with Applications 42(24),\\n9603–9611 (2015)\\n[96] Nourallah, M.: One size does not fit all: Young retail investors’ initial\\ntrust in financial robo-advisors. Journal of Business Research 156, 113470\\n(2023)\\n[97] Orderud, F.: Comparison of kalman filter estimation approaches for state\\nspace models with nonlinear measurements. In: Proc. of Scandinavian\\nConference on Simulation and Modeling. pp. 1–8 (2005)\\n[98] Paulson, F.L., et al.: What makes a portfolio a portfolio?. Educational\\nleadership 48(5), 60–63 (1991)\\n[99] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improv-\\ning language understanding by generative pre-training (2018)\\n[100] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.:\\nLanguage models are unsupervised multitask learners. OpenAI blog 1(8),\\n9 (2019)\\n[101] Radha, S., Thenmozhi, M.: Forecasting short term interest rates using\\narma, arma-garch and arma-egarch models. In: Indian Institute of Capital\\nMarkets 9th Capital Markets Conference Paper (2006)\\n[102] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,\\nZhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning\\nwith a unified text-to-text transformer. The Journal of Machine Learning\\nResearch 21(1), 5485–5551 (2020)\\n[103] Rezayi, S., Dai, H., Liu, Z., Wu, Z., Hebbar, A., Burns, A.H., Zhao, L.,\\nZhu, D., Li, Q., Liu, W., et al.: Clinicalradiobert: Knowledge-infused few\\nshot learning for clinical notes named entity recognition. In: International\\nWorkshop on Machine Learning in Medical Imaging. pp. 269–278. Springer\\n(2022)\\n[104] Rezayi, S., Liu, Z., Wu, Z., Dhakal, C., Ge, B., Dai, H., Mai, G., Liu,\\nN., Zhen, C., Liu, T., et al.: Exploring new frontiers in agricultural nlp:\\nInvestigating the potential of large language models for food applications.\\narXiv preprint arXiv:2306.11892 (2023)\\n33\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='2a0eec18-4f8c-40f5-81ff-ae95444b98ad', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '34'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[105] Rezayi, S., Liu, Z., Wu, Z., Dhakal, C., Ge, B., Zhen, C., Liu, T., Li, S.:\\nAgribert: knowledge-infused agricultural language models for matching\\nfood and nutrition. In: Proceedings of the Thirty-First International Joint\\nConference on Artificial Intelligence. vol. 7, pp. 5150–5156 (2022)\\n[106] Roy, A., Sun, J., Mahoney, R., Alonzi, L., Adams, S., Beling, P.: Deep\\nlearning detecting fraud in credit card transactions. In: 2018 systems and\\ninformation engineering design symposium (SIEDS). pp. 129–134. IEEE\\n(2018)\\n[107] Scherer, B., Lehner, S.: Trust me, i am a robo-advisor. Journal of Asset\\nManagement 24(2), 85–96 (2023)\\n[108] Sert, O.C., S¸ahin, S.D., ¨Ozyer, T., Alhajj, R.: Analysis and prediction\\nin sparse and high dimensional text data: The case of dow jones stock\\nmarket. Physica A: Statistical Mechanics and its Applications 545, 123752\\n(2020)\\n[109] Shah, D., Isah, H., Zulkernine, F.: Predicting the effects of news senti-\\nments on the stock market. In: 2018 IEEE International Conference on\\nBig Data (Big Data). pp. 4705–4708. IEEE (2018)\\n[110] Shah, R.S., Chawla, K., Eidnani, D., Shah, A., Du, W., Chava, S., Raman,\\nN., Smiley, C., Chen, J., Yang, D.: When flue meets flang: Benchmarks\\nand large pre-trained language model for financial domain. arXiv preprint\\narXiv:2211.00083 (2022)\\n[111] Shi, Y., Xu, S., Liu, Z., Liu, T., Li, X., Liu, N.: Mededit: Model editing\\nfor medical question answering with external knowledge bases (2023)\\n[112] Soun, Y., Yoo, J., Cho, M., Jeon, J., Kang, U.: Accurate stock movement\\nprediction with self-supervised learning from sparse noisy tweets. In: 2022\\nIEEE International Conference on Big Data (Big Data). pp. 1691–1700.\\nIEEE (2022)\\n[113] Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y.,\\nLyu, W., Zhang, Y., Li, X., et al.: Trustllm: Trustworthiness in large\\nlanguage models. arXiv preprint arXiv:2401.05561 (2024)\\n[114] Tan, C., Cao, Q., Li, Y., Zhang, J., Yang, X., Zhao, H., Wu, Z., Liu,\\nZ., Yang, H., Wu, N., et al.: On the promises and challenges of multi-\\nmodal foundation models for geographical, environmental, agricultural,\\nand urban planning applications. arXiv preprint arXiv:2312.17016 (2023)\\n[115] Tang, C., Liu, Z., Ma, C., Wu, Z., Li, Y., Liu, W., Zhu, D., Li, Q., Li, X.,\\nLiu, T., Fan, L.: Policygpt: Automated analysis of privacy policies with\\nlarge language models (2023)\\n[116] Tang, R., Chuang, Y.N., Hu, X.: The science of detecting llm-generated\\ntexts. arXiv preprint arXiv:2303.07205 (2023)\\n34\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='cedf05e0-b22a-49d8-afed-14d94797b46d', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '35'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[117] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A.,\\nLacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al.:\\nLlama: Open and efficient foundation language models. arXiv preprint\\narXiv:2302.13971 (2023)\\n[118] Usha, M., Devi, M.I.: Analysis of sentiments using unsupervised learning\\ntechniques. In: 2013 International Conference on Information Communi-\\ncation and Embedded Systems (ICICES). pp. 241–245. IEEE (2013)\\n[119] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\\nA.N., Kaiser,  L., Polosukhin, I.: Attention is all you need. Advances in\\nneural information processing systems 30 (2017)\\n[120] Visintin, A., Conti, M.: Leveraging social networks for mergers and ac-\\nquisitions forecasting. In: International Conference on Web Information\\nSystems Engineering. pp. 144–159. Springer (2023)\\n[121] Wang, J., Liu, Z., Zhao, L., Wu, Z., Ma, C., Yu, S., Dai, H., Yang, Q.,\\nLiu, Y., Zhang, S., Shi, E., Pan, Y., Zhang, T., Zhu, D., Li, X., Jiang,\\nX., Ge, B., Yuan, Y., Shen, D., Liu, T., Zhang, S.: Review of large vision\\nmodels and visual prompt engineering (2023)\\n[122] Wang, J., Wu, Z., Li, Y., Jiang, H., Shu, P., Shi, E., Hu, H., Ma, C., Liu,\\nY., Wang, X., et al.: Large language models for robotics: Opportunities,\\nchallenges, and perspectives. arXiv preprint arXiv:2401.04334 (2024)\\n[123] Wang, S., Yuan, H., Zhou, L., Ni, L.M., Shum, H.Y., Guo, J.: Alpha-gpt:\\nHuman-ai interactive alpha mining for quantitative investment (2023)\\n[124] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,\\nZhou, D., et al.: Chain-of-thought prompting elicits reasoning in large\\nlanguage models. Advances in Neural Information Processing Systems 35,\\n24824–24837 (2022)\\n[125] Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S.,\\nKambadur, P., Rosenberg, D., Mann, G.: Bloomberggpt: A large language\\nmodel for finance (2023)\\n[126] Wu, T., He, S., Liu, J., Sun, S., Liu, K., Han, Q.L., Tang, Y.: A brief\\noverview of chatgpt: The history, status quo and potential future develop-\\nment. IEEE/CAA Journal of Automatica Sinica 10(5), 1122–1136 (2023)\\n[127] Wu, Z., Zhang, L., Cao, C., Yu, X., Dai, H., Ma, C., Liu, Z., Zhao, L.,\\nLi, G., Liu, W., Li, Q., Shen, D., Li, X., Zhu, D., Liu, T.: Exploring the\\ntrade-offs: Unified large language models vs local fine-tuned models for\\nhighly-specific radiology nli task (2023)\\n[128] Xie, Q., Han, W., Zhang, X., Lai, Y., Peng, M., Lopez-Lira, A., Huang,\\nJ.: Pixiu: A large language model, instruction data and evaluation bench-\\nmark for finance (2023)\\n35\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='f57a7821-7fc5-4510-96e8-03ab64f74d87', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '36'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[129] Yang, H., Liu, X.Y., Wang, C.D.: Fingpt: Open-source financial large\\nlanguage models. arXiv preprint arXiv:2306.06031 (2023)\\n[130] Yang, L., Ng, T.L.J., Smyth, B., Dong, R.:\\nFact check:\\nAnalyz-\\ning financial events from multilingual news sources. arXiv preprint\\narXiv:2106.15221 (2021)\\n[131] Yang, Y., Uy, M.C.S., Huang, A.: Finbert: A pretrained language model\\nfor financial communications. arXiv preprint arXiv:2006.08097 (2020)\\n[132] Yoon, S.: Design and implementation of an llm system to improve re-\\nsponse time for smes technology credit evaluation. International journal\\nof advanced smart convergence 12(3), 51–60 (2023)\\n[133] Yu, J.H., Kang, J., Park, S.: Information availability and return volatility\\nin the bitcoin market: analyzing differences of user opinion and interest.\\nInformation Processing & Management 56(3), 721–732 (2019)\\n[134] Yu, X., Chen, Z., Ling, Y., Dong, S., Liu, Z., Lu, Y.: Temporal data\\nmeets llm – explainable financial time series forecasting (2023)\\n[135] Yu, Y., Li, H., Chen, Z., Jiang, Y., Li, Y., Zhang, D., Liu, R., Suchow,\\nJ.W., Khashanah, K.: Finme: A performance-enhanced large language\\nmodel trading agent with layered memory and character design. arXiv\\npreprint arXiv:2311.13743 (2023)\\n[136] Zhang, B., Yang, H., Zhou, T., Ali Babar, M., Liu, X.Y.: Enhancing fi-\\nnancial sentiment analysis via retrieval augmented large language models.\\nIn: Proceedings of the Fourth ACM International Conference on AI in\\nFinance. pp. 349–356 (2023)\\n[137] Zhao, H., Ling, Q., Pan, Y., Zhong, T., Hu, J.Y., Yao, J., Xiao, F., Xiao,\\nZ., Zhang, Y., Xu, S.H., et al.: Ophtha-llama2: A large language model\\nfor ophthalmology. arXiv preprint arXiv:2312.04906 (2023)\\n[138] Zhao, L., Zhang, L., Wu, Z., Chen, Y., Dai, H., Yu, X., Liu, Z., Zhang,\\nT., Hu, X., Jiang, X., et al.: When brain-inspired ai meets agi. Meta-\\nRadiology p. 100005 (2023)\\n[139] Zhao, L., Li, L., Zheng, X., Zhang, J.: A bert based sentiment analysis\\nand key entity detection approach for online financial texts. In: 2021 IEEE\\n24th International Conference on Computer Supported Cooperative Work\\nin Design (CSCWD). pp. 1233–1238. IEEE (2021)\\n[140] Zhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y.,\\nZhang, B., Zhang, J., Dong, Z., et al.: A survey of large language models.\\narXiv preprint arXiv:2303.18223 (2023)\\n[141] Zheng, W., Jin, M.: The effects of class imbalance and training data size\\non classifier learning: an empirical study. SN Computer Science 1, 1–13\\n(2020)\\n36\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='68fe4f6b-a22b-4e72-95a5-dca24ad6fdb8', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '37'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, text='[142] Zheng, Z., Chen, K.Y., Cao, X.Y., Lu, X.Z., Lin, J.R.: Llm-funcmapper:\\nFunction identification for interpreting complex clauses in building codes\\nvia llm. arXiv preprint arXiv:2308.08728 (2023)\\n[143] Zhong, T., Wei, Y., Yang, L., Wu, Z., Liu, Z., Wei, X., Li, W., Yao, J.,\\nMa, C., Li, X., et al.: Chatabl: Abductive learning via natural language\\ninteraction with chatgpt. arXiv preprint arXiv:2304.11107 (2023)\\n[144] Zhou, M., Liu, X., Liu, D., Wu, Z., Liu, Z., Zhao, L., Zhu, D., Guo,\\nL., Han, J., Liu, T., et al.:\\nFine-grained artificial neurons in audio-\\ntransformers for disentangling neural auditory encoding. In: Findings of\\nthe Association for Computational Linguistics: ACL 2023. pp. 7943–7956\\n(2023)\\n[145] Zivot, E., Wang, J.: Vector autoregressive models for multivariate time\\nseries. Modeling financial time series with S-PLUS® pp. 385–429 (2006)\\n[146] Zumente, I., L¯ace, N.: Esg rating—necessity for the investor or the com-\\npany? sustainability, 13 (16), 8940 (2021)\\n37\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "                                    similarity_top_k=5\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What are the different usecases where LLMs can help in finance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response=' LLMs can be used in finance for tasks such as automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. LLMs can also be used to distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction.', source_nodes=[NodeWithScore(node=TextNode(id_='8aa0b97b-5c9c-466e-9322-e889658b3401', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '3'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c1bdd320-34c9-4590-9f9a-bc943fc1cc74', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '3'}, hash='039d12776a5427a97014f6e3f03a36a65f20b55cc24cfd3b175ac0fef6024ce2'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='eb69b781-91b4-4949-b33c-bb5ef6e72c31', node_type=<ObjectType.TEXT: '1'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '2'}, hash='ad5d27cea1dca7b15bd2d132bb4823789e4b477c626afb639d95e32eed77fd2f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='b7367eb3-48ae-46a7-a230-f0fd7264fe50', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bfbe71585b13436f8fc814636ae3eae878aacccafbffec108d14dd8a91192fc9')}, text='reliable [42].\\nTo address these issues, researchers and developers are continuously refining\\nthe algorithms of LLMs to improve its understanding and processing of special-\\nized domain knowledge. With a large amount of specialized training data, the\\nmodel can better grasp specific knowledge in the financial domain. At the same\\ntime, the combination of expert systems and manual review mechanisms can\\nfurther improve the accuracy and reliability of the model’s application in the\\nfinancial domain [131].\\nOverall, large-scale language models are gradually becoming a powerful tool\\nfor dealing with financial problems.\\nThey are not only able to process and\\nanalyze large amounts of data, but also provide in-depth insights and recom-\\nmendations [125]. Although there are some challenges in the application process,\\nthey are gradually being overcome as technology continues to advance. Looking\\nahead, the application of LLMs in the financial sector will undoubtedly open up\\nmore innovations and opportunities.\\nIn this review, we tackle the key question of how to address the difficul-\\nties inherent in the financial sector while utilizing the successes of LLMs from\\nvarious fields to enhance the finance industry. The significant contributions of\\nthis article are distilled into four primary points, each focusing on the synergy\\nbetween LLMs and financial applications.\\n• We meticulously survey and synthesize existing LLMs for finance liter-\\nature, exploring the latest advancements in four independent task cate-\\ngories: financial engineering, financial forecasting, financial risk manage-\\nment, and financial real-time question answering.\\n• We summarize the primary technical approaches that LLMs offer to the\\nrealm of finance, examine the potential in the investment field, and provide\\na foundational survey for researchers in this domain.\\n• We assess the effectiveness of GPT-4 in various tasks.\\n• We concisely overview of the most significant results from our research,\\ndiscuss the major unresolved issues that should be addressed in subsequent\\nefforts, and offer insights into future directions and possibilities in this\\nfield..\\n2\\nRelated Work\\n2.1\\nLarge Language Models\\nLLMs are primarily built upon the Transformer architecture [119], which has\\nbeen central to their ability to handle complex language tasks. The Transformer\\nmodel is structured with two key components: the Encoder and the Decoder,\\neach consisting of multiple layers of self-attention and feed-forward neural net-\\nworks. This architecture facilitates effective management of long-range depen-\\ndencies within sequences.\\n3', start_char_idx=0, end_char_idx=2592, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.771534498006833), NodeWithScore(node=TextNode(id_='eb69b781-91b4-4949-b33c-bb5ef6e72c31', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '2'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='12a09a5a-62bf-46f0-bef8-1f8aff941cab', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '2'}, hash='d488f2a1e429e06eeabb603a92c93faccd2396fd0822d4d256059efd4b7c6f9b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='374d4264-77f4-4126-a53f-026884d6e115', node_type=<ObjectType.TEXT: '1'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '1'}, hash='4cba18ad7e85e7c6effc028efd2fefa5dcb20ee5033b2eff651c42cf549ce042'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8aa0b97b-5c9c-466e-9322-e889658b3401', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='4407bf2e8a7609f84c31bedeb8a26dd00f02d4a3f3cf6ffc4d431ac0c9a24f5a')}, text='Figure 1: An overview of the LLMs’ capacities in financial tasks.\\n1\\nIntroduction\\nOver the past few years, LLMs such as OpenAI’s GPT family have made sig-\\nnificant advances in the field of natural language processing (NLP). The de-\\nvelopment of these models marks an important milestone in AI technology for\\nunderstanding and generating natural language. With increased computational\\npower and improved algorithms, LLMs has demonstrated amazing capabilities\\nin understanding complex contexts, answering questions, and writing content.\\nEspecially in the finance domain, these capabilities of LLMs are gradually show-\\ning their great potential [18, 125, 126].\\nFinance is a highly specialized and complex field that involves a great deal\\nof data analysis, prediction, and decision making.\\nLLM’s ability to process\\nlarge-scale text data makes it a promising application in the financial field. For\\nexample, by analyzing financial reports, market news, investor communications,\\netc., LLMs can provide insights into market trends, perform risk assessments,\\nand even assist in investment decisions. In addition, LLMs can process natural\\nlanguage queries and provide instant financial advice and support, which is a\\nbig step forward for the financial services industry [28, 66, 125].\\nHowever, applying LLMs to the financial sector also faces several challenges.\\nFirst, data in the financial domain is highly specialized and complex. Financial\\nterminology, regulations, and market dynamics require a high level of model\\ncomprehension. In addition, financial decision-making usually involves high risk,\\nwhich requires a high degree of accuracy and reliability in prediction. Therefore,\\nit is a major challenge to ensure that the output of LLMs is both accurate and\\n2', start_char_idx=0, end_char_idx=1755, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7709608068909372), NodeWithScore(node=TextNode(id_='374d4264-77f4-4126-a53f-026884d6e115', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '1'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='80b1bce8-ba7c-423f-8feb-2651bef2b723', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '1'}, hash='acc67093e340f380300d88e6bcc3b4d9493743beaef086081a72a9424ad57f95'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='eb69b781-91b4-4949-b33c-bb5ef6e72c31', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='313d1476ad1296c0ca088c95d17921bcd51cfdaa086c4b86a25e8fe478149b67')}, text='Revolutionizing Finance with LLMs: An\\nOverview of Applications and Insights\\nHuaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang,\\nPeng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai,\\nNinghao Liu, Tianming Liu†∗†\\nJanuary 23, 2024\\nAbstract\\nIn recent years, Large Language Models (LLMs) like ChatGPT have\\nseen considerable advancements and have been applied in diverse fields.\\nBuilt on the Transformer architecture, these models are trained on exten-\\nsive datasets, enabling them to understand and generate human language\\neffectively. In the financial domain, the deployment of LLMs is gaining\\nmomentum. These models are being utilized for automating financial re-\\nport generation, forecasting market trends, analyzing investor sentiment,\\nand offering personalized financial advice. Leveraging their natural lan-\\nguage processing capabilities, LLMs can distill key insights from vast fi-\\nnancial data, aiding institutions in making informed investment choices\\nand enhancing both operational efficiency and customer satisfaction. In\\nthis study, we provide a comprehensive overview of the emerging inte-\\ngration of LLMs into various financial tasks. Additionally, we conducted\\nholistic tests on multiple financial tasks through the combination of natu-\\nral language instructions. Our findings show that GPT-4 effectively follow\\nprompt instructions across various financial tasks. This survey and evalu-\\nation of LLMs in the financial domain aim to deepen the understanding of\\nLLMs’ current role in finance for both financial practitioners and LLM re-\\nsearchers, identify new research and application prospects, and highlight\\nhow these technologies can be leveraged to solve practical challenges in\\nthe finance industry.\\n∗†Corresponding authors: Tianming Liu\\n†Huanqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen\\nXu, Haixing Dai, Lin Zhao, Ninghao Liu, Tianming are with the School of Computing, The\\nUniversity of Georgia, Athens 30602, USA. Gengchen Mai is with the Department of Geogra-\\nphy, University of Georgia, Athens 30602, USA.\\n1\\narXiv:2401.11641v1  [cs.CL]  22 Jan 2024', start_char_idx=0, end_char_idx=2116, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7447760425733515), NodeWithScore(node=TextNode(id_='fa7121a9-5a26-48f6-9c1f-78e38befa290', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '25'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c8c3c6d3-a506-4e3e-b568-907894900d7d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '25'}, hash='16a6806d9492bb4872193c1220e4efe63080c5dfd7e5c811e64f813fdf0cc674'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c8df06fc-2202-47cb-8ec9-97bab9a12c59', node_type=<ObjectType.TEXT: '1'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '24'}, hash='800b346d5561ffe36e79e22359f55f7fbb33838d7936ea95a57664aee3953f7d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9a2befb6-b308-435a-bcf2-240abd718b20', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a9957566203b1cd9a92dbb411c01ab0d591c54566bea81bc2655a64fb98d3fc2')}, text='straints of LLMs in the financial domain.\\nCentral to our findings is the re-\\nmarkable adeptness of LLMs in text processing, sentiment analysis, and their\\nzero-shot learning abilities. The proficiency of LLMs in sifting through and in-\\nterpreting extensive textual data is unmatched, thus playing a pivotal role in\\ndecoding market dynamics and investor sentiment.\\nHowever, it is crucial to acknowledge the limitations of LLMs in direct com-\\nputational tasks, particularly in optimization and quantitative trading, where\\ntheir role remains largely supplementary. Despite these constraints, the poten-\\ntial of LLMs in enhancing financial models and decision-making processes is\\nundeniable. As we advance, the integration of LLMs with quantitative models\\nand the refinement of their application in finance will be areas of significant\\ninterest. The continual evolution of LLMs promises to not only bolster existing\\nfinancial methodologies but also to pave the way for innovative approaches in\\nfinancial analysis and strategy.\\nReferences\\n[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L.,\\nAlmeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4\\ntechnical report. arXiv preprint arXiv:2303.08774 (2023)\\n[2] Ahmadi, Z., Martens, P., Koch, C., Gottron, T., Kramer, S.: Towards\\nbankruptcy prediction: Deep sentiment mining to detect financial distress\\nfrom business management reports. In: 2018 IEEE 5th International Con-\\nference on Data Science and Advanced Analytics (DSAA). pp. 293–302.\\nIEEE (2018)\\n[3] Ali, S.R.M.: Geopolitical threat, market capitalization, and portfolio re-\\nturn. Market Capitalization, and Portfolio Return\\n[4] Alvarado, J.C.S., Verspoor, K., Baldwin, T.: Domain adaption of named\\nentity recognition to support credit risk assessment. In: Proceedings of\\nthe Australasian Language Technology Association Workshop 2015. pp.\\n84–90 (2015)\\n[5] Araci, D.: Finbert: Financial sentiment analysis with pre-trained lan-\\nguage models. arXiv preprint arXiv:1908.10063 (2019)\\n[6] Audrino, F., Sigrist, F., Ballinari, D.: The impact of sentiment and at-\\ntention measures on stock market volatility. International Journal of Fore-\\ncasting 36(2), 334–357 (2020)\\n[7] Aw, E.C.X., Leong, L.Y., Hew, J.J., Rana, N.P., Tan, T.M., Jee, T.W.:\\nCounteracting dark sides of robo-advisors: justice, privacy and intrusion\\nconsiderations. International Journal of Bank Marketing (2023)\\n[8] Babaei, G., Giudici, P.: Gpt classifications, with application to credit\\nlending. Available at SSRN 4649285 (2023)\\n25', start_char_idx=0, end_char_idx=2534, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7382334350120203), NodeWithScore(node=TextNode(id_='b7367eb3-48ae-46a7-a230-f0fd7264fe50', embedding=None, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '4'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='09c44a6f-d1c8-44bf-821c-93a687fc66b7', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '4'}, hash='85d43a7df6a808988eccfd55854f29b524bde49a81c1d1ec07f58ab8bbb8d551'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8aa0b97b-5c9c-466e-9322-e889658b3401', node_type=<ObjectType.TEXT: '1'>, metadata={'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '3'}, hash='c25223449b7421d5d1d423c286eaf5de26f83b6018c0a904fcc58a84c43d5cde'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='7f0dca48-4446-43ab-85b9-91a495e8a34f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='bb715305d0228b3c33c84584b317740d2ef06e9f61136207fd60de7adb8f121c')}, text='Figure 2: The Ability of LLMs in Financial Tasks\\nAttention(q, k, v) = softmax( qk\\n√dk\\n)v\\n(1)\\nSelf-attention is characterized by its use of queries (Q), keys (K), and values\\n(V), three vectors derived from the input data.\\nEach element in the input\\nsequence is transformed into these three vectors through linear transformation.\\nThe self-attention mechanism then computes the attention scores by taking the\\ndot product of the query with all keys. These scores determine how much focus\\nor ’attention’ each element in the sequence should have in relation to every\\nother element. The attention scores are normalized using a softmax function,\\nensuring they sum up to one, thus forming a probability distribution. The final\\noutput of the self-attention layer is a weighted sum of the value vectors, where\\nthe weights are the softmax-normalized attention scores. This process allows\\neach output element of the self-attention layer to be a combination of the inputs,\\nwith the weights specifying the amount of attention given to each input element.\\nThe self-attention mechanism’s ability to weigh inputs differently allows LLMs\\nto capture complex relationships in the data, such as long-range dependencies,\\nmaking it exceptionally powerful for tasks that require an understanding of\\ncontext and sequence.\\nThe architecture of LLMs typically falls into one of two categories: Decoder-\\nonly and Encoder-Decoder. Decoder-only models, such as those in the GPT\\nseries [99, 100], generate text in a unidirectional manner [12]. Each token in\\nthe input sequence attends only to preceding tokens, making them well-suited\\nfor tasks like text generation. The Encoder-Decoder models, like T5 [102] and\\nBART [56], feature separate mechanisms for encoding input sequences and de-\\ncoding them into target sequences. This design allows them to handle a broader\\nrange of tasks, including both generation and comprehension.\\nToken generation in LLMs is a vital process, involving vocabulary creation,\\nprobability prediction, and techniques like beam search for sequence generation.\\n4', start_char_idx=0, end_char_idx=2053, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7369540525179591)], metadata={'8aa0b97b-5c9c-466e-9322-e889658b3401': {'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '3'}, 'eb69b781-91b4-4949-b33c-bb5ef6e72c31': {'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '2'}, '374d4264-77f4-4126-a53f-026884d6e115': {'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '1'}, 'fa7121a9-5a26-48f6-9c1f-78e38befa290': {'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '25'}, 'b7367eb3-48ae-46a7-a230-f0fd7264fe50': {'total_pages': 37, 'file_path': './docs/finance.pdf', 'source': '4'}})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LLMs can be used in finance for tasks such as automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. LLMs can also be used to distill key insights from vast financial data, aiding institutions in making informed investment choices and enhancing both operational efficiency and customer satisfaction.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: 8aa0b97b-5c9c-466e-9322-e889658b3401): reliable [42].\\nTo address these issues, researchers and developers are continuously refining\\nthe ...\\n\\n> Source (Doc id: eb69b781-91b4-4949-b33c-bb5ef6e72c31): Figure 1: An overview of the LLMs’ capacities in financial tasks.\\n1\\nIntroduction\\nOver the past fe...\\n\\n> Source (Doc id: 374d4264-77f4-4126-a53f-026884d6e115): Revolutionizing Finance with LLMs: An\\nOverview of Applications and Insights\\nHuaqin Zhao, Zhenglia...\\n\\n> Source (Doc id: fa7121a9-5a26-48f6-9c1f-78e38befa290): straints of LLMs in the financial domain.\\nCentral to our findings is the re-\\nmarkable adeptness o...\\n\\n> Source (Doc id: b7367eb3-48ae-46a7-a230-f0fd7264fe50): Figure 2: The Ability of LLMs in Financial Tasks\\nAttention(q, k, v) = softmax( qk\\n√dk\\n)v\\n(1)\\nSelf...'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
